import os
import sys
import openai
import time
import json
import requests
import random
import django
import pandas as pd
from bs4 import BeautifulSoup
from typing import List, Dict, Tuple
from django.conf import settings
from django.contrib.auth.models import User
from quiz_room.models import Article # ê° ì‚¬ìš©ìì˜ ê³¼ê±° ì•„í‹°í´ ì¤‘ë³µ ì œê±°

# Django í”„ë¡œì íŠ¸ ì ˆëŒ€ ê²½ë¡œë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..') )) # __file__ : í˜„ì¬ ê²½ë¡œ

# DJANGO_SETTINGS_MODULE í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ Django ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myquiz.settings')

# í‚¤ ì„¤ì • 
openai.api_key = settings.OPENAI_API_KEY 
GOOGLE_SEARCH_ENGINE_ID  = settings.GOOGLE_SEARCH_ENGINE_ID
GOOGLE_API_KEY = settings.GOOGLE_API_KEY


'''
extract_keywords
'''

# ëœë¤ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords() -> Tuple[List, str]:
    max_keywords=1  # ìµœëŒ€ ì¶”ì¶œ í‚¤ì›Œë“œ ìˆ˜ ì„¤ì •
    fail_cnt = 0  # ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
    categories = [
        # ì‚¬íšŒ
        "ì œë„", "ê¶Œë ¥", "ê³„ì¸µ", "ë¬¸í™”", "ê²½ì œ", "ì •ì¹˜", "ê³µë™ì²´", "ë…¸ë™", "êµìœ¡", "ë²•",
        "ë³µì§€", "ê¸°ìˆ ", "ë¯¸ë””ì–´", "í™˜ê²½", "ì  ë”", "ê°€ì¡±", "ë²”ì£„", "ìœ¤ë¦¬", "ì´ì£¼", "ì„¸ê³„í™”",
    
        # ê³¼í•™
        "ìì—°", "ìš°ì£¼", "ìƒëª…", "ì—ë„ˆì§€", "ë¬¼ì§ˆ", "ê¸°ìˆ ", "ì‹¤í—˜", "ì§„í™”", "í™˜ê²½", "ìˆ˜í•™",
        "í™”í•™", "ë¬¼ë¦¬í•™", "ìƒë¬¼í•™", "ì§€êµ¬ê³¼í•™", "ì²œë¬¸í•™", "ì¸ê³µì§€ëŠ¥", "ìœ ì „", "ì‹ ê²½ê³¼í•™", "ì „ìê¸°", "ë°©ì‚¬ëŠ¥",
    
        # ì—­ì‚¬
        "ë¬¸ëª…", "êµ­ê°€", "ì „ìŸ", "í˜ëª…", "ì‚°ì—…", "ì‚¬íšŒ", "ë¬¸í™”", "ì •ì¹˜", "ê²½ì œ", "êµë¥˜",
        "ì¢…êµ", "ì™•ì¡°", "ì œêµ­", "ë¬´ì—­", "íƒí—˜", "ì‹ë¯¼ì§€", "ë…ë¦½", "ì„¸ê³„ì‚¬", "ë¯¼ì¡±", "ì™¸êµ",
    
        # ì² í•™
        "ì¡´ì¬", "ì¸ì‹", "ì§„ë¦¬", "ìœ¤ë¦¬", "ë…¼ë¦¬", "ì¸ê°„", "ììœ ", "ê²½í—˜", "ì‚¬ìƒ", "ê°€ì¹˜",
        "ì‹ ë…", "ë„ë•", "ì–¸ì–´", "ì •ì˜", "ì‹¤ì¬", "ì´ì„±", "ê°ì„±", "ê·¼ëŒ€ì„±", "ì´ˆì›”", "ëª©ì ",
    
        # ì¢…êµ
        "ì‹ ì•™", "ì‹ í™”", "ê²½ì „", "ì˜ë¡€", "ì‹ ì„±", "êµ¬ì›", "ì˜ì„±", "ì‹ í•™", "êµë¦¬", "ì´ˆì›”",
        "ì°½ì¡°", "ì˜ˆë°°", "ê¸°ë„", "ì„±ì§", "ì‹ ë¹„", "ê³„ì‹œ", "ì‹ ë…", "ìœ¤ë¦¬", "ë‚´ì„¸", "ì„±ìŠ¤ëŸ¬ì›€",
    
        # ì˜ˆìˆ 
        "ë¯¸í•™", "í‘œí˜„", "ì°½ì‘", "ê°ì •", "í˜•ì‹", "ìƒì§•", "ì•„ë¦„ë‹¤ì›€", "ì˜ê°", "ì¡°í™”", "ìƒìƒë ¥",
        "ìƒ‰ì±„", "ê³µê°„", "ì›€ì§ì„", "ì†Œë¦¬", "ì¡°í˜•", "ì„œì‚¬", "í•´ì„", "ë…ì°½ì„±", "ìŠ¤íƒ€ì¼", "íŒ¨í„´"
    ]
    random_category = random.choice(categories)
    print("ğŸ” ëœë¤ ì¹´í…Œê³ ë¦¬:", random_category)

    while fail_cnt < 3:
        try:
            system_prompt = f"""
            ë‹¹ì‹ ì˜ ì—­í• ì€ SEO(ê²€ìƒ‰ ì—”ì§„ ìµœì í™”)ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
            1. **í‚¤ì›Œë“œ ìƒì„± ì¡°ê±´**:
              - ì¹´í…Œê³ ë¦¬ëŠ” {random_category}ì…ë‹ˆë‹¤.
              - ì´ ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ëœ êµì–‘ ì§€ì‹ ìŠµë“ì— ìœ ìš©í•œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
              - í‚¤ì›Œë“œ ìƒì„± ì‹œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­:
                - **ì •ë³´ì„±**: ì‚¬ëŒë“¤ì´ í•™ìŠµí•˜ê±°ë‚˜ íƒêµ¬í•˜ê³ ì í•  ë•Œ ìœ ìš©í•œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
                - **ê²€ìƒ‰ ê°€ëŠ¥ì„±**: ì‚¬ëŒë“¤ì´ ê²€ìƒ‰í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, íƒ€ê²Ÿ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í• ë§Œí•œ í‚¤ì›Œë“œë¥¼ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ê²€ìƒ‰ ê°€ëŠ¥ì„±ì„ ìœ„í•´ì„œ ê°€ëŠ¥í•œ êµ¬ì²´ì ì´ì§„ ì•Šê³  ì¼ë°˜ì ì´ê³  í¬ê´„ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
            2. **í‚¤ì›Œë“œ ìƒì„± ê·œì¹™**:
              - ê²€ìƒ‰ ì—”ì§„ì—ì„œ ìì£¼ ê²€ìƒ‰ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”.
              - ëª…ì‚¬ ì¤‘ì‹¬ì˜ êµ¬ì²´ì ì´ê³  ì§ê´€ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
              - ë™ì¼í•œ ë‹¨ì–´ë‚˜ ì¤‘ë³µëœ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ì„¸ìš”.
              - í‚¤ì›Œë“œì˜ ê°œìˆ˜ëŠ” {max_keywords}ê°œ ì…ë‹ˆë‹¤

            3. **ì¶œë ¥ í˜•ì‹**:
              - ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”. 
              - í‚¤ì›Œë“œ ê°œìˆ˜ì— ë§ê²Œ ë”•ì…”ë„ˆë¦¬ë¥¼ êµ¬ì„±í•˜ì„¸ìš”.
                ì˜ˆ:
                ```
                  {{"k1":"í‚¤ì›Œë“œ1", "k2":"í‚¤ì›Œë“œ2", "k3":"í‚¤ì›Œë“œ3", ...}}
                ```
            """

            # Open API í˜¸ì¶œ
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    { "role": "user",
                      "content": ("í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."),
                    },
                ],
                temperature=0.5, # ì¼ê´€ì„±
                max_tokens=2048,
                top_p=1, # ì¶”ì¶œ ë‹¨ì–´ì˜ ê´€ë ¨ì„± ì œì–´
                frequency_penalty=0, # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë¥¼ ì œì–´(ê³ ë ¤)
                presence_penalty=0, # ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì œì–´(ê³ ë ¤)
            )

            # GPTì˜ ì‘ë‹µ ë¶„ì„ 
            keywords = response["choices"][0]["message"]["content"]

            # (JSON -> ë”•ì…”ë„ˆë¦¬) ë³€í™˜
            keywords = keywords.replace("'", "\"") # JSON í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
            try:
                keywords_dict = json.loads(keywords)
                keywords_list = list(keywords_dict.values()) # (ë”•ì…”ë„ˆë¦¬ value -> ë¦¬ìŠ¤íŠ¸) ë³€í™˜
            except json.JSONDecodeError as e:
                fail_cnt += 1
                print(f"ğŸ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì‘ë‹µ ë‚´ìš©: {response['choices'][0]['message']['content']}")
                continue  # ì¬ì‹œë„: whileë¬¸ ì²˜ìŒë¶€í„°

            query = " ".join(map(str, keywords_list))
            print("ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ:", keywords_list) # ë””ë²„ê¹… 
            # ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ 
            return (keywords_list, query)
        
        except openai.error.RateLimitError:
            fail_cnt += 1
            print("ğŸ” Rate limitì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. 40ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
            time.sleep(40)
        except Exception as e:
            print(f"ğŸ” Error during OpenAI API call: {e}")
            return ([], "")
        
        print("âš ï¸ 3ë²ˆ ì´ìƒì˜ ì‹¤íŒ¨ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return ([],"")  # 3ë²ˆ ì´ìƒ ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜





'''
    select_article
        - Google_API
        - process_recommend_article
            - get_article_body
            - find_recommend_article
            - Google_API
        
'''
# ì‚¬ìš©ì ì…ë ¥ 
def select_article(player_1:User, player_2:User, query:str) -> Dict:

    num_results_per_site = 3    # ê° ì‚¬ì´íŠ¸ë‹¹ ê²°ê³¼ ê°œìˆ˜
    sites = [                   # ê²€ìƒ‰ ê°€ëŠ¥ ì‚¬ì´íŠ¸ ëª©ë¡ 
        # "brunch.co.kr",
        "bbc.com",
        "khan.co.kr",
        "hani.co.kr",
        "ytn.co.kr",
        "sisain.co.kr",
        "news.sbs.co.kr",
        "h21.hani.co.kr",
        "ohmynews.com",
    ]

    # í›„ë³´ ê¸°ì‚¬ ëª©ë¡ ì„œì¹˜ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê¸°ë°˜ ì¿¼ë¦¬ë¡œ)
    df = Google_API(player_1, player_2, query, num_results_per_site, sites)  # queryë¡œ íƒìƒ‰ëœ ê¸°ì‚¬ ëª©ë¡
    # print(df) # ë””ë²„ê¹… # í›„ë³´ ê¸°ì‚¬ ëª©ë¡
    time.sleep(30)  # ìƒì„± í† í° ì œí•œ ì—ëŸ¬ ì˜ˆë°©

    
    # ì¶”ì²œ ì•„í‹°í´ ê²°ì • 
    extracted_keywords = None # ì´ˆê¸°í™” # í‚¤ì›Œë“œ ì¬ìƒì„± ì‹œ í™œìš©
    attempts = 0
    while attempts <= 3: # 3íšŒ ì´ˆê³¼í•˜ë©´ ì¤‘ë‹¨
        # ì¶”ì²œ ì•„í‹°í´
        info_for_the_article = process_recommend_article(df, query)
        # print(f"ğŸ”ì¶”ì²œ ì•„í‹°í´: {info_for_the_article}") # ë””ë²„ê¹…
        
        if info_for_the_article is None or info_for_the_article.empty: # ì¶”ì²œëœ ì•„í‹°í´ì´ ì—†ê±°ë‚˜ ë³¸ë¬¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•  ê²½ìš°
            attempts += 1
            # ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìƒì„±í•˜ì—¬ ì¿¼ë¦¬(ê²€ìƒ‰ì–´) ì¬êµ¬ì„±
            extracted_keywords = extract_keywords() # í‚¤ì›Œë“œ ì¬ì¬ì¶”ì¶œ
            if extracted_keywords:
                query = " ".join(map(str, extracted_keywords)) # ì¶”ì¶œëœ í‚¤ì›Œë“œ ì €ì¥(ê¸°ì¡´ í‚¤ì›Œë“œ ì‚­ì œ & ìƒˆë¡œìš´ ê²€ìƒ‰ì–´ ì„¤ì •)
            else:
                query = None

            # Google APIë¡œ ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰
            df = Google_API(player_1, player_2, query, num_results_per_site=5, sites=sites)
            if df.empty: # ìƒˆë¡œìš´ ê²€ìƒ‰ì–´ë¡œë„ ê²°ê³¼ë¥¼ ì°¨ì§€ ëª»í•¨
                continue  # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ë°˜ë³µ
        else: # ì¶”ì²œ ì•„í‹°í´ì´ ì¡´ì¬í•œë‹¤ë©´
            # Title, URL ë° Body ì¶”ì¶œ
            recommend_article_title = info_for_the_article.iloc[0]["Title"]
            recommend_article_body = info_for_the_article.iloc[0]["Body"]
            recommend_article_url = info_for_the_article.iloc[0]["URL"]

            # ë³¸ë¬¸ì´ ìœ íš¨í•œì§€ í™•ì¸
            # IndexError: single positional indexer is out-of-bounds -> recommend_article_body (DataFrame)ì´ ë¹ˆ ê²½ìš° ì¢…ì¢… ë°œìƒ!
            if recommend_article_body and len(recommend_article_body.strip()) > 0:
                print("ğŸ” ì¶”ì²œ ì•„í‹°í´ URL:", recommend_article_url)
                break  # ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
    
    if attempts >= 3:
        print("âš ï¸ ì•„í‹°í´ ì¶”ì²œì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤.")
        return {
            "title": "",
            "body": "", 
            "url": "",
            "retry_extracted_keywords": "" # í‚¤ì›Œë“œ ì¬ì¶”ì¶œì‹œ # í•„ìš” ì—†ìœ¼ë©´ ì œê±°
        }
    
    return  {
        "title": recommend_article_title,
        "body": recommend_article_body, 
        "url": recommend_article_url,
        "retry_extracted_keywords": extracted_keywords # í‚¤ì›Œë“œ ì¬ì¶”ì¶œì‹œ # í•„ìš” ì—†ìœ¼ë©´ ì œê±°
    }


        



# í›„ë³´ ê¸°ì‚¬ë“¤ ("Title", "Description", "Link", "Domain") í˜•ì‹ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ
def Google_API(player_1:User, player_2:User, query:str, num_results_per_site:int, sites:list[str]) -> pd.DataFrame: # DataFrame:2ì°¨ì› ë°ì´í„° êµ¬ì¡° (í–‰&ì—´ êµ¬ì„±)
    # ê° ì‚¬ì´íŠ¸ì˜ ê²°ê³¼ ëª¨ìŒ ë¦¬ìŠ¤íŠ¸(ì¶”ì²œ ê¸°ì‚¬ í›„ë³´ ë¦¬ìŠ¤íŠ¸)
    df_google_list = [] # ìš”ì†ŒëŠ” dataframeìœ¼ë¡œ í•œ ê¸°ì‚¬ì˜ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ

    # ì‚¬ìš©ìì˜ ê³¼ê±° ì•„í‹°í´ ì •ë³´ # ìœ ì € ì •ë³´ íŒŒë¼ë¯¸í„° í•„ìš”
    player_1_past_articles = set(Article.objects.filter(user=player_1).order_by('-timestamp')[:100].values_list('url', flat=True)) # player_1ì˜ ê³¼ê±° ì•„í‹°í´ì„ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ í›„, ìƒìœ„ 100ê°œ ë°˜í™˜(ì¤‘ë³µì€ ì‚­ì œ)
    player_2_past_articles = set(Article.objects.filter(user=player_2).order_by('-timestamp')[:100].values_list('url', flat=True)) # plaeyr_2ì˜ ê³¼ê±° ì•„í‹°í´ì„ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ í›„, ìƒìœ„ 100ê°œ ë°˜í™˜(ì¤‘ë³µì€ ì‚­ì œ)
    past_articles = player_1_past_articles.union(player_2_past_articles)

    # ì‚¬ì´íŠ¸ ë³„ ì¿¼ë¦¬ë¡œ í›„ë³´ ê¸°ì‚¬ ì¡°íšŒ
    for site in sites: 
        
        site_query = f"site:{site} {query}"     # ê° ì‚¬ì´íŠ¸ ë³„ ê²€ìƒ‰ì–´ êµ¬ì„±
        collected_results_cnt = 0               # í˜„ì¬ ì‚¬ì´íŠ¸ì—ì„œ ìˆ˜ì§‘í•œ ê²°ê³¼ ìˆ˜
        start_index = 1                         # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ íƒìƒ‰ ì‹œì‘í•  ìœ„ì¹˜ë¥¼ ì§€ì •
        num = 5                                # í•œ ë²ˆì— ë°˜í™˜í•  ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆ˜
        # Google Custom Search APIì— ëŒ€í•œ ìš”ì²­ URL ìƒì„±
        url = f"https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={GOOGLE_SEARCH_ENGINE_ID}&q={site_query}&start={start_index}&num={num}" 
            
        while collected_results_cnt < num_results_per_site: # ì •í•´ì§„ ê°œìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ    
            # ìš”ì²­
            try:
                # URLë¡œ HTTP GET ìš”ì²­
                response = requests.get(url)

                # ìš”ì²­ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                if response.status_code != 200: # Get ìš”ì²­ ì‹¤íŒ¨
                    print(f"âš ï¸ {site}ì— ëŒ€í•œ HTTP GET ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, Message: {response.text}")
                    break # while ë£¨í”„ ì¢…ë£Œ
                
                # API ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬(ìš”ì²­ ì„±ê³µì‹œ ì‘ë™)
                data = response.json() # APIì˜ ì‘ë‹µ ë°ì´í„° -> JSON í˜•ì‹
                # print(f"get data: {data}") # ë””ë²„ê¹…ìš© # êµ¬ì¡° í™•ì¸ìš© 
                
                # ê²€ìƒ‰ ê²°ê³¼ í•­ëª© ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                search_items = data.get("items") # ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ë“¤ ê°€ì ¸ì˜¤ê¸°
                if not search_items: # ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´
                    print(f"ğŸ” No more results found for site {site}.")
                    break # while ë£¨í”„ ì¢…ë£Œ

                # ê²€ìƒ‰ ê²°ê³¼ ìˆœíšŒ(ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ)
                for search_item in search_items:
                    # ê²°ê³¼ ê°œìˆ˜ ì²´í¬
                    if collected_results_cnt >= num_results_per_site: # ì •í•´ì§„ ê°œìˆ˜ì— ë„ë‹¬í•œ ê²½ìš°
                        break # for ë£¨í”„ ì¢…ë£Œ

                    # ì •ë³´ ì¶”ì¶œ
                    link = search_item.get("link")
                    title = search_item.get("title")
                    description = search_item.get("snippet")

                    # ì‚¬ìš©ì ê³¼ê±° ì•„í‹°í´ê³¼ì˜ ì¤‘ë³µ ë°©ì§€
                    if link in past_articles:
                        continue  

                    # ì¶”ê°€ ì¡°ê±´: m.khan.co.kr ì²˜ë¦¬
                    if site == "khan.co.kr" and "m.khan.co.kr" in link:
                        link = link.replace("m.khan.co.kr", "khan.co.kr") # (ëª¨ë°”ì¼ ë§í¬ -> ì¼ë°˜ ì›¹ì‚¬ì´íŠ¸ ë§í¬)ë³€í™˜

                    # ê¸°ì‚¬ ì •ë³´ì— ëŒ€í•œ DataFrame ìƒì„± í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    df_google_list.append(
                        pd.DataFrame(
                            [[title, description, link, site]], # ë‹¤ìŒ í–‰ ì¶”ê°€(í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ 1ê°œì˜ í–‰ êµ¬ì„±)
                            columns=["Title", "Description", "Link", "Domain"], # ì—´ ì •ë³´
                        )
                    )
                    collected_results_cnt += 1  # í˜„ì¬ ìˆ˜ì§‘ ê²°ê³¼ ê°œìˆ˜ ì¦ê°€
                # ë‹¤ìŒ í˜ì´ì§€ ê²€ìƒ‰
                if num_results_per_site > 10: # num_results_per_siteê°€ 10 ì´ìƒì¼ ë•Œë§Œ
                    start_index += 10 # start_indexë¥¼ ì¦ê°€ì‹œí‚´(numì„ 10ìœ¼ë¡œ ì„¤ì •í•´ë’€ê¸° ë•Œë¬¸)

            except Exception as e:
                print(f"âš ï¸ Error occurred for site {site}: {e}")
                break

    # ëª¨ë“  ì‚¬ì´íŠ¸ì˜ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
    if df_google_list: 
        # df_google_listì˜ ìš”ì†Œì¸ ê° DataFrameë“¤ì€ ëª¨ë‘ ë™ì¼í•œ ì—´ êµ¬ì¡°ì´ë¯€ë¡œ, í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•© ê°€ëŠ¥
        df_google = pd.concat(df_google_list, ignore_index=True)  # ignore_index=True: ìƒˆë¡œìš´ ì—°ì†ì ì¸ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬
    else:  # df_google_listê°€ ë¹„ì–´ìˆë‹¤ë©´
        df_google = pd.DataFrame(columns=["Title", "Description", "Link", "Domain"]) # ë¹ˆ DataFrame 

    return df_google
   



# ì¶”ì²œëœ ì•„í‹°í´ì—ì„œ URL, Body, Titleì„ ì¶”ì¶œ
    # ì¶”ì²œëœ ì•„í‹°í´ì´ ì—†ê±°ë‚˜, ì¶”ì²œëœ ì•„í‹°í´ì˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì„ ê²½ìš° 
    # í•´ë‹¹ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œìš´ ì¶”ì²œì„ ìš”ì²­í•˜ëŠ” í•¨ìˆ˜
def process_recommend_article(df:pd.DataFrame=None, query:str="") -> pd.DataFrame:
    # ì´ˆê¸°í™”
    recommend_article = pd.DataFrame(columns=["Title", "Description", "Link", "Domain"])
 
    # ì¶”ì²œ ì •ë³´ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤
    while True:
        # ì¶”ì²œ ì•„í‹°í´ íƒìƒ‰
        recommend_article = find_recommend_article(df, query) 

        # ì•„í‹°í´ ì¡´ì¬ ì—¬ë¶€ íŒŒì•…
        if recommend_article.empty: # ì¶”ì²œëœ ì•„í‹°í´ì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            print("ğŸ” ì¶”ì²œëœ ì•„í‹°í´ì´ ë” ì´ìƒ ì—†ìŠµë‹ˆë‹¤.")
            return None #  while ë£¨í”„ ì¢…ë£Œ

        try: # ì¶”ì²œ ì•„í‹°í´ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            # URL, Domain, Title ì¶”ì¶œ
            url = recommend_article.iloc[0]["Link"]  # URL ì¶”ì¶œ
            domain = recommend_article.iloc[0]["Domain"]  # Domain ì¶”ì¶œ
            title = recommend_article.iloc[0]["Title"]  # Title ì¶”ì¶œ
        except IndexError as e:
            print(f"ğŸ” ì¶”ì²œëœ ì•„í‹°í´ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            recommend_article.drop(recommend_article.index[0], inplace=True) # recommend_articleì—ì„œ ì‚­ì œ
            df.drop(df[df["Link"] == url].index, inplace=True) # dfì—ì„œ ì‚­ì œ
            continue  # ìƒˆë¡œìš´ ì•„í‹°í´ì„ íƒìƒ‰ì„ ìœ„í•´ while ë£¨í”„ ì¬ì‹œì‘

        # ë³¸ë¬¸(article body) ì¶”ì¶œ
        article_body = get_article_body(url, domain)  # ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ
        
        if ( # ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ ë³¸ë¬¸ ê¸¸ì´ê°€ 5ë¬¸ì¥ ì´í•˜ì¸ ê²½ìš° ì²˜ë¦¬
            not article_body
            or len([s for s in article_body.split(".") if s.strip()]) <= 5
        ):
            print(f"ğŸ” ë³¸ë¬¸ì´ ì—†ëŠ” ì•„í‹°í´ (ë˜ëŠ” ë³¸ë¬¸ì´ 5ë¬¸ì¥ ì´í•˜)ì´ë¥´ëª¨ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            recommend_article.drop(recommend_article.index[0], inplace=True)# recommend_articleì—ì„œ ì‚­ì œ
            df.drop(df[df["Link"] == url].index, inplace=True) # dfì—ì„œ ì‚­ì œ
            continue   # ìƒˆë¡œìš´ ì•„í‹°í´ì„ íƒìƒ‰ì„ ìœ„í•´ while ë£¨í”„ ì¬ì‹œì‘

        # ë³¸ë¬¸ì´ ìœ íš¨í•  ê²½ìš° DataFrame ìƒì„± ë° ë°˜í™˜
        info_for_the_article = pd.DataFrame(
            [[title, url, article_body]],
            columns=["Title", "URL", "Body"]
        )
    
        return info_for_the_article
    


# ì¶”ì²œ ì•„í‹°í´ ê²°ì •#
def find_recommend_article(df_google:pd.DataFrame, query:str="") -> pd.DataFrame:
    # ì•„í‹°í´ ëª©ë¡ì— index í¬í•¨
    article_titles = df_google["Title"].tolist()
    article_descriptions = df_google["Description"].tolist()
    article_indices = df_google.index.tolist()  # DataFrameì˜ indexë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    # print(f"find_recommend_article: {query}") # ë””ë²„ê¹…
    fail = 0

    while fail < 3:
        try:
            # Open API í˜¸ì¶œ
                # í† í° ì´ˆê³¼ ì—ëŸ¬ ë°œìƒí•´ì„œ, title ì •ë³´ëŠ” ì œì™¸í•¨!
            system_prompt = f"""
            # ì§€ì‹œë¬¸
                - ë‹¹ì‹ ì€ ì•„í‹°í´ì˜ í‚¤ì›Œë“œì¸ queryì™€ ì•„í‹°í´ì˜ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì•„í‹°í´ì„ ì¶”ì²œí•˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì˜ ì—­í• ì„ í•œë‹¤.
            # ì¶”ì²œ ì¡°ê±´
                1. queryì—ì„œ ë‹¤ë£¨ëŠ” ì£¼ì œì™€ ê´€ë ¨ì´ ìˆëŠ” ì•„í‹°í´ì„ ì„ íƒí•˜ì„¸ìš”.
                2. ì œëª©ê³¼ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„í‹°í´ì˜ ì í•©ì„±ì„ íŒë‹¨í•˜ì„¸ìš”.
                3. ë‹¨ìˆœ ë‰´ìŠ¤ ë³´ë„, ê´‘ê³ ì„± ë‚´ìš©, ë˜ëŠ” ì¤‘ë³µëœ ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”.
                4. ì§€ì‹ì ì¸ ì„¤ëª… ë˜ëŠ” í•™ìŠµì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.

            # ì¶œë ¥ í˜•ì‹
              - index ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”
                ì˜ˆ:
                ```
                    6
                ```
            """
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": (
                            f"query: {query}\n\n"
                            "ì•„í‹°í´ ëª©ë¡ (index í¬í•¨):\n"
                            + "\n".join(
                                f"{i}. [Index: {idx}]  ì„¤ëª…: {description}\n  "
                                for i, (idx, description) in enumerate(
                                    zip(
                                        article_indices,
                                        article_descriptions,
                                    )
                                )
                            )
                        ),
                    },
                ],
                temperature=0,
                max_tokens=2048,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0,
            )


            # GPTì˜ ì‘ë‹µ ë¶„ì„ 
            content = response["choices"][0]["message"]["content"]
            # print(f"ğŸ” ì¶”ì²œ ì•„í‹°í´ì˜ ì •ë³´ í™•ì¸\n index: {content}") # ë””ë²„ê¹…

            # ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜ ì‹œë„
            try:
                recommended_index = int(content)  # ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜
            except ValueError:
                recommended_index  = 0  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •

            # indexê°€ DataFrameì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if recommended_index not in df_google.index:
                print(f"âš ï¸ ì¶”ì²œëœ index({recommended_index})ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            # í•´ë‹¹ indexë¡œ í–‰(í•´ë‹¹ ê¸°ì‚¬ ì •ë³´) ë°˜í™˜
            recommended_article = df_google.loc[[recommended_index]] 
            return recommended_article # ê²°ê³¼ DataFrame í˜•íƒœë¡œ ë°˜í™˜

        except openai.error.RateLimitError:
            print("ğŸ” Rate limit reached. Retrying in 40 seconds...")
            time.sleep(40)  # 40ì´ˆ ì§€ì—° í›„ ì¬ì‹œë„
            fail += 3
            continue
    

def get_article_body(url:str, domain:str) -> str:
    # ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì´íŠ¸ë³„ íƒœê·¸ ì •ë³´#
    SITE_CLASS_MAPPING = {
        # "brunch.co.kr": [{"tag": "div", "class": "wrap_body"}],
        "bbc.com": [{"tag": "main", "class": "bbc-fa0wmp"}],
        "khan.co.kr": [{"tag": "div", "class": "art_body"}],
        "hani.co.kr": [{"tag": "div", "class": "article-text"}],
        "ytn.co.kr": [{"tag": "div", "class": "vodtext"}],
        "sisain.co.kr": [{"tag": "div", "class": "article-body"}],
        "news.sbs.co.kr": [{"tag": "div", "class": "main_text"}],
        "h21.hani.co.kr": [{"tag": "div", "class": "arti-txt"}],
        "ohmynews.com": [
            {"tag": "article", "class": "article_body at_contents article_view"},
            {"tag": "div", "class": "news_body"},
        ],
        # ê³µë°± ì£¼ì˜ # ì¶”ê°€ ë„ë©”ì¸ ë° íƒœê·¸/í´ë˜ìŠ¤ ë§¤í•‘
        # í•œêµ­ì¼ë³´, mbcnewsëŠ” ì œì™¸
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    }

    try:
        # URLì—ì„œ HTML (ìš”ì²­)ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, headers=headers)
        response.raise_for_status() # ì˜¤ë¥˜ ë°œìƒí•˜ë©´, ì˜ˆì™¸ ë°œìƒ
        soup = BeautifulSoup(response.text, "html.parser")

        # ë„ë©”ì¸ì´ ì œê³µëœ ê²½ìš° SITE_CLASS_MAPPINGì—ì„œ ì²˜ë¦¬
        site_info = SITE_CLASS_MAPPING.get(domain)
        if not site_info: # í•´ë‹¹ ë„ë©”ì¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° 
            return None

        # ëª¨ë“  ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒí•˜ë©° íƒœê·¸/í´ë˜ìŠ¤ ì²˜ë¦¬
        for mapping in site_info:
            tag_name = mapping.get("tag")
            class_name = mapping.get("class")
            main_body = soup.find(tag_name, class_=class_name)
            if main_body:
                # íƒœê·¸ ë‚´ë¶€ì— p, h1 ë“±ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
                text_elements = main_body.find_all(["h1", "h2", "h3", "h4", "p", "li"])
                # <p> íƒœê·¸ ê°œìˆ˜ í™•ì¸ (2ê°œ ì´í•˜ì´ë©´ ë³¸ë¬¸ì´ ë¶€ì¡±í•˜ë‹¤ê³  ê°„ì£¼)

                paragraph_count = len(main_body.find_all("p"))
                if paragraph_count <= 2:
                    return main_body.get_text(strip=True)

                if text_elements:
                    return "\n".join(
                        [element.get_text(strip=True) for element in text_elements]
                    )
                else:
                    return main_body.get_text(strip=True)
        # í•´ë‹¹ ë„ë©”ì¸ì— ë§¤í•‘ëœ íƒœê·¸ì™€ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° 
        return None 
    except requests.exceptions.RequestException as e: # HTTP ìš”ì²­ ì˜¤ë¥˜
        return None
    except Exception as e: # ë³¸ë¬¸ ì¶”ì¶œì¤‘ ì˜¤ë¥˜
        return None

