import os
import sys
import openai
import time
import json
import requests
import random
import django
import pandas as pd
from bs4 import BeautifulSoup
from typing import List, Dict, Tuple
from django.conf import settings
from django.contrib.auth.models import User
from quiz_room.models import Article # ê° ì‚¬ìš©ìì˜ ê³¼ê±° ì•„í‹°í´ ì¤‘ë³µ ì œê±°

# Django í”„ë¡œì íŠ¸ ì ˆëŒ€ ê²½ë¡œë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..') )) # __file__ : í˜„ì¬ ê²½ë¡œ

# DJANGO_SETTINGS_MODULE í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ Django ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myquiz.settings')

# í‚¤ ì„¤ì • 
openai.api_key = settings.OPENAI_API_KEY 
GOOGLE_SEARCH_ENGINE_ID  = settings.GOOGLE_SEARCH_ENGINE_ID
GOOGLE_API_KEY = settings.GOOGLE_API_KEY


'''
extract_keywords
'''

# ëœë¤ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords() -> Tuple[List, str]:
    max_keywords=1  # ìµœëŒ€ ì¶”ì¶œ í‚¤ì›Œë“œ ìˆ˜ ì„¤ì •
    fail_cnt = 0  # ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
    categories = [
        # ì‚¬íšŒ
        "ì œë„", "ê¶Œë ¥", "ê³„ì¸µ", "ë¬¸í™”", "ê²½ì œ", "ì •ì¹˜", "ê³µë™ì²´", "ë…¸ë™", "êµìœ¡", "ë²•",
        "ë³µì§€", "ê¸°ìˆ ", "ë¯¸ë””ì–´", "í™˜ê²½", "ì  ë”", "ê°€ì¡±", "ë²”ì£„", "ìœ¤ë¦¬", "ì´ì£¼", "ì„¸ê³„í™”",
    
        # ê³¼í•™
        "ìì—°", "ìš°ì£¼", "ìƒëª…", "ì—ë„ˆì§€", "ë¬¼ì§ˆ", "ê¸°ìˆ ", "ì‹¤í—˜", "ì§„í™”", "í™˜ê²½", "ìˆ˜í•™",
        "í™”í•™", "ë¬¼ë¦¬í•™", "ìƒë¬¼í•™", "ì§€êµ¬ê³¼í•™", "ì²œë¬¸í•™", "ì¸ê³µì§€ëŠ¥", "ìœ ì „", "ì‹ ê²½ê³¼í•™", "ì „ìê¸°", "ë°©ì‚¬ëŠ¥",
    
        # ì—­ì‚¬
        "ë¬¸ëª…", "êµ­ê°€", "ì „ìŸ", "í˜ëª…", "ì‚°ì—…", "ì‚¬íšŒ", "ë¬¸í™”", "ì •ì¹˜", "ê²½ì œ", "êµë¥˜",
        "ì¢…êµ", "ì™•ì¡°", "ì œêµ­", "ë¬´ì—­", "íƒí—˜", "ì‹ë¯¼ì§€", "ë…ë¦½", "ì„¸ê³„ì‚¬", "ë¯¼ì¡±", "ì™¸êµ",
    
        # ì² í•™
        "ì¡´ì¬", "ì¸ì‹", "ì§„ë¦¬", "ìœ¤ë¦¬", "ë…¼ë¦¬", "ì¸ê°„", "ììœ ", "ê²½í—˜", "ì‚¬ìƒ", "ê°€ì¹˜",
        "ì‹ ë…", "ë„ë•", "ì–¸ì–´", "ì •ì˜", "ì‹¤ì¬", "ì´ì„±", "ê°ì„±", "ê·¼ëŒ€ì„±", "ì´ˆì›”", "ëª©ì ",
    
        # ì¢…êµ
        "ì‹ ì•™", "ì‹ í™”", "ê²½ì „", "ì˜ë¡€", "ì‹ ì„±", "êµ¬ì›", "ì˜ì„±", "ì‹ í•™", "êµë¦¬", "ì´ˆì›”",
        "ì°½ì¡°", "ì˜ˆë°°", "ê¸°ë„", "ì„±ì§", "ì‹ ë¹„", "ê³„ì‹œ", "ì‹ ë…", "ìœ¤ë¦¬", "ë‚´ì„¸", "ì„±ìŠ¤ëŸ¬ì›€",
    
        # ì˜ˆìˆ 
        "ë¯¸í•™", "í‘œí˜„", "ì°½ì‘", "ê°ì •", "í˜•ì‹", "ìƒì§•", "ì•„ë¦„ë‹¤ì›€", "ì˜ê°", "ì¡°í™”", "ìƒìƒë ¥",
        "ìƒ‰ì±„", "ê³µê°„", "ì›€ì§ì„", "ì†Œë¦¬", "ì¡°í˜•", "ì„œì‚¬", "í•´ì„", "ë…ì°½ì„±", "ìŠ¤íƒ€ì¼", "íŒ¨í„´"
    ]
    random_category = random.choice(categories)
    print("ğŸ” ëœë¤ ì¹´í…Œê³ ë¦¬:", random_category)

    while fail_cnt < 3:
        try:
            system_prompt = f"""
            ë‹¹ì‹ ì˜ ì—­í• ì€ SEO(ê²€ìƒ‰ ì—”ì§„ ìµœì í™”)ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
            1. **í‚¤ì›Œë“œ ìƒì„± ì¡°ê±´**:
              - ì¹´í…Œê³ ë¦¬ëŠ” {random_category}ì…ë‹ˆë‹¤.
              - ì´ ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ëœ êµì–‘ ì§€ì‹ ìŠµë“ì— ìœ ìš©í•œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
              - í‚¤ì›Œë“œ ìƒì„± ì‹œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­:
                - **ì •ë³´ì„±**: ì‚¬ëŒë“¤ì´ í•™ìŠµí•˜ê±°ë‚˜ íƒêµ¬í•˜ê³ ì í•  ë•Œ ìœ ìš©í•œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
                - **ê²€ìƒ‰ ê°€ëŠ¥ì„±**: ì‚¬ëŒë“¤ì´ ê²€ìƒ‰í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, íƒ€ê²Ÿ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í• ë§Œí•œ í‚¤ì›Œë“œë¥¼ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ê²€ìƒ‰ ê°€ëŠ¥ì„±ì„ ìœ„í•´ì„œ ê°€ëŠ¥í•œ êµ¬ì²´ì ì´ì§„ ì•Šê³  ì¼ë°˜ì ì´ê³  í¬ê´„ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
            2. **í‚¤ì›Œë“œ ìƒì„± ê·œì¹™**:
              - ê²€ìƒ‰ ì—”ì§„ì—ì„œ ìì£¼ ê²€ìƒ‰ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”.
              - ëª…ì‚¬ ì¤‘ì‹¬ì˜ êµ¬ì²´ì ì´ê³  ì§ê´€ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
              - ë™ì¼í•œ ë‹¨ì–´ë‚˜ ì¤‘ë³µëœ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ì„¸ìš”.
              - í‚¤ì›Œë“œì˜ ê°œìˆ˜ëŠ” {max_keywords}ê°œ ì…ë‹ˆë‹¤

            3. **ì¶œë ¥ í˜•ì‹**:
              - ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”. 
              - í‚¤ì›Œë“œ ê°œìˆ˜ì— ë§ê²Œ ë”•ì…”ë„ˆë¦¬ë¥¼ êµ¬ì„±í•˜ì„¸ìš”.
                ì˜ˆ:
                ```
                  {{"k1":"í‚¤ì›Œë“œ1", "k2":"í‚¤ì›Œë“œ2", "k3":"í‚¤ì›Œë“œ3", ...}}
                ```
            """

            # Open API í˜¸ì¶œ
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    { "role": "user",
                      "content": ("í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."),
                    },
                ],
                temperature=0.5, # ì¼ê´€ì„±
                max_tokens=2048,
                top_p=1, # ì¶”ì¶œ ë‹¨ì–´ì˜ ê´€ë ¨ì„± ì œì–´
                frequency_penalty=0, # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë¥¼ ì œì–´(ê³ ë ¤)
                presence_penalty=0, # ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì œì–´(ê³ ë ¤)
            )

            # GPTì˜ ì‘ë‹µ ë¶„ì„ 
            keywords = response["choices"][0]["message"]["content"]

            # (JSON -> ë”•ì…”ë„ˆë¦¬) ë³€í™˜
            keywords = keywords.replace("'", "\"") # JSON í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
            try:
                keywords_dict = json.loads(keywords)
                keywords_list = list(keywords_dict.values()) # (ë”•ì…”ë„ˆë¦¬ value -> ë¦¬ìŠ¤íŠ¸) ë³€í™˜
            except json.JSONDecodeError as e:
                fail_cnt += 1
                print(f"ğŸ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì‘ë‹µ ë‚´ìš©: {response['choices'][0]['message']['content']}")
                continue  # ì¬ì‹œë„: whileë¬¸ ì²˜ìŒë¶€í„°

            query = " ".join(map(str, keywords_list))
            print("ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ:", keywords_list) # ë””ë²„ê¹… 
            # ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ 
            return (keywords_list, query)
        
        except openai.error.RateLimitError:
            fail_cnt += 1
            print("ğŸ” Rate limitì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. 40ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
            time.sleep(40)
        except Exception as e:
            print(f"ğŸ” Error during OpenAI API call: {e}")
            return ([], "")
        
        print("âš ï¸ 3ë²ˆ ì´ìƒì˜ ì‹¤íŒ¨ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return ([],"")  # 3ë²ˆ ì´ìƒ ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜





'''
    select_article
        - Google_API
        - process_recommend_article
            - get_article_body
            - find_recommend_article
            - Google_API
        
'''
# ì‚¬ìš©ì ì…ë ¥ 
def select_article(player_1:User, player_2:User, query:str) -> Dict:
    fail = 0                    
    retry_extracted_keywords = None # í‚¤ì›Œë“œ ì¬ì¶”ì¶œ ì‹œ
    num_results_per_site = 3    # ê° ì‚¬ì´íŠ¸ë‹¹ ê²°ê³¼ ê°œìˆ˜
    sites = [                   # ê²€ìƒ‰ ê°€ëŠ¥ ì‚¬ì´íŠ¸ ëª©ë¡ 
        # "brunch.co.kr",
        "bbc.com",
        "khan.co.kr",
        "hani.co.kr",
        "ytn.co.kr",
        "sisain.co.kr",
        "news.sbs.co.kr",
        "h21.hani.co.kr",
        "ohmynews.com",
    ]

    # 1. í›„ë³´ ê¸°ì‚¬ ëª©ë¡ ì„œì¹˜ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê¸°ë°˜ ì¿¼ë¦¬ë¡œ)
    df = Google_API(player_1, player_2, query, num_results_per_site, sites)  # queryë¡œ íƒìƒ‰ëœ ê¸°ì‚¬ ëª©ë¡
    time.sleep(20)  # ìƒì„± í† í° ì œí•œ ì—ëŸ¬ ì˜ˆë°©

    
    # 2. ì¶”ì²œ ì•„í‹°í´ ê²°ì • 
    while fail < 3:
        # ì•„í‹°í´ ì¶”ì²œ gpt ìš”ì²­
        info_for_the_article = process_recommend_article(df, query)
        
        if info_for_the_article is None : # ì•„í‹°í´ ì¶”ì²œ ì‹¤íŒ¨
            fail += 1
            # í‚¤ì›Œë“œ ì¬ì¶”ì¶œ -> ê²€ìƒ‰ ì¿¼ë¦¬ ì¬êµ¬ì„±
            retry_extracted_keywords = extract_keywords()
            if retry_extracted_keywords:
                query = " ".join(retry_extracted_keywords) # ê²€ìƒ‰ ì¿¼ë¦¬ ì¬êµ¬ì„±
                df = Google_API(player_1, player_2, query, num_results_per_site=5, sites=sites) # í›„ë³´ ê¸°ì‚¬ ëª©ë¡ ì¬êµ¬ì„±
        else: # ì¶”ì²œ ì•„í‹°í´ì´ ì¡´ì¬í•œë‹¤ë©´
            # Title, URL ë° Body ì¶”ì¶œ
            recommend_article_title = info_for_the_article["Title"]
            recommend_article_body = info_for_the_article["Body"]
            recommend_article_url = info_for_the_article["URL"]
            print("âœ… ì¶”ì²œ ì•„í‹°í´ URL:", recommend_article_url)
            break  # ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
    
    # 3. ìµœì¢… ì¶”ì²œ ì•„í‹°í´ ì •ë³´ ë°˜í™˜
    return  {
        "title": recommend_article_title,
        "body": recommend_article_body, 
        "url": recommend_article_url, 
        "retry_extracted_keywords": retry_extracted_keywords # í‚¤ì›Œë“œ ì¬ì¶”ì¶œì‹œ, DBì— ë°˜ì˜í•˜ê¸° ìœ„í•¨ 
    }


        



# í›„ë³´ ê¸°ì‚¬ ì •ë³´ ("Title", "Description", "Link", "Domain") í˜•ì‹
def Google_API(player_1:User, player_2:User, query:str, num_results_per_site:int, sites:list[str]) -> pd.DataFrame: # DataFrame:2ì°¨ì› í–‰&ì—´ ë°ì´í„° êµ¬ì¡°
     # í›„ë³´ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    df_google_list = [] 

    # ì‚¬ìš©ìì˜ ê³¼ê±° ì•„í‹°í´ ì •ë³´
    player_1_past_articles = set(Article.objects.filter(user=player_1).order_by('-timestamp')[:100].values_list('url', flat=True)) # player_1ì˜ ê³¼ê±° ì•„í‹°í´ì„ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ í›„, ìƒìœ„ 100ê°œ ë°˜í™˜(ì¤‘ë³µì€ ì‚­ì œ)
    player_2_past_articles = set(Article.objects.filter(user=player_2).order_by('-timestamp')[:100].values_list('url', flat=True)) # plaeyr_2ì˜ ê³¼ê±° ì•„í‹°í´ì„ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ í›„, ìƒìœ„ 100ê°œ ë°˜í™˜(ì¤‘ë³µì€ ì‚­ì œ)
    past_articles = player_1_past_articles.union(player_2_past_articles)

    # ì‚¬ì´íŠ¸ ë³„ ì¿¼ë¦¬ë¡œ í›„ë³´ ê¸°ì‚¬ ì¡°íšŒ
    for site in sites: 
        # 1. Google Custom Search APIì— ëŒ€í•œ ìš”ì²­ URL êµ¬ì„±
        site_query = f"site:{site} {query}"     # ê° ì‚¬ì´íŠ¸ ë³„ ê²€ìƒ‰ì–´ êµ¬ì„±
        collected_results_cnt = 0               # ìˆ˜ì§‘í•œ ê²°ê³¼ ê°œìˆ˜
        start_index = 1                         # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ íƒìƒ‰ ì‹œì‘ ìœ„ì¹˜
        num = 5                                 # í•œ ë²ˆì— ë°˜í™˜í•  ê²°ê³¼ì˜ ê°œìˆ˜
        url = f"https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={GOOGLE_SEARCH_ENGINE_ID}&q={site_query}&start={start_index}&num={num}" 
            
        while collected_results_cnt < num_results_per_site: # ì •í•´ì§„ ê°œìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€
            try:
                # 2. URLë¡œ HTTP GET ìš”ì²­
                response = requests.get(url)

                # 3. ìš”ì²­ ì„±ê³µ ì—¬ë¶€ì— ë”°ë¥¸ ì²˜ë¦¬ 
                if response.status_code != 200: # ìš”ì²­ ì‹¤íŒ¨
                    print(f"âš ï¸ {site}ì— ëŒ€í•œ HTTP GET ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, Message: {response.text}")
                    break 
                else: # ìš”ì²­ ì„±ê³µ 
                    data = response.json() # ì‘ë‹µ ë°ì´í„° -> JSON í˜•ì‹
                    search_items = data.get("items") # ê²€ìƒ‰ ê²°ê³¼ í•­ëª©(ê¸°ì‚¬)ë“¤ ê°€ì ¸ì˜¤ê¸°
                    if not search_items: # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´
                        print(f"ğŸ” '{site}'ì— ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break 

                # 4. ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                for search_item in search_items:
                    # ê²°ê³¼ ê°œìˆ˜ ì²´í¬
                    if collected_results_cnt >= num_results_per_site: # ê²°ê³¼ ê°œìˆ˜ ë„ë‹¬
                        break # for ë£¨í”„ ì¢…ë£Œ

                    # ì •ë³´ ì¶”ì¶œ
                    link, title, description = search_item.get("link"), search_item.get("title"), search_item.get("snippet")

                    # ì‚¬ìš©ì ê³¼ê±° ì•„í‹°í´ê³¼ì˜ ì¤‘ë³µ ë°©ì§€
                    if link in past_articles:
                        continue  

                    # ì¶”ê°€ ì¡°ê±´: m.khan.co.kr ì²˜ë¦¬
                    if site == "khan.co.kr" and "m.khan.co.kr" in link:
                        link = link.replace("m.khan.co.kr", "khan.co.kr") # (ëª¨ë°”ì¼ ë§í¬ -> ì¼ë°˜ ì›¹ì‚¬ì´íŠ¸ ë§í¬)ë³€í™˜

                    # ê¸°ì‚¬ ì •ë³´ì— ëŒ€í•œ DataFrame ìƒì„± í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    df_google_list.append(
                        pd.DataFrame(
                            [[title, description, link, site]], # ë‹¤ìŒ í–‰ ì¶”ê°€(í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ 1ê°œì˜ í–‰ êµ¬ì„±)
                            columns=["Title", "Description", "Link", "Domain"], # ì—´ ì •ë³´
                        )
                    )
                    collected_results_cnt += 1  # í˜„ì¬ ìˆ˜ì§‘ ê²°ê³¼ ê°œìˆ˜ ì¦ê°€
            except Exception as e:
                print(f"âš ï¸ Error occurred for site {site}: {e}")
                break
    # 5. ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
    if df_google_list: 
        df_google = pd.concat(df_google_list, ignore_index=True)  # ignore_index=True: ìƒˆë¡œìš´ ì—°ì†ì ì¸ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬
    else:  # df_google_listê°€ ë¹„ì–´ìˆë‹¤ë©´
        df_google = pd.DataFrame(columns=["Title", "Description", "Link", "Domain"]) # ë¹ˆ DataFrame 

    return df_google



# gpt ê¸°ë°˜ ì•„í‹°í´ ì¶”ì²œ í›„, body ì •ë³´ ì¶”ì¶œí•˜ì—¬ ì¶”ì²œ ê¸°ì‚¬ ì •ë³´ ë°˜í™˜í™˜ 
    # ì¶”ì²œëœ ì•„í‹°í´ì´ ì—†ê±°ë‚˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì„ ê²½ìš°, í•´ë‹¹ ë°ì´í„°ë¥¼ ì‚­ì œ í›„ ì¬ì¶”ì²œ
def process_recommend_article(df:pd.DataFrame=None, query:str="") -> Dict:
    # ì´ˆê¸°í™”
    fail = 0

    # ì¶”ì²œ ì•„í‹°í´ ì„ ì • í›„, body ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤
    while fail < 3:
        # 1. ì¶”ì²œ ì•„í‹°í´ ë°˜í™˜
        recommend_article =  find_recommend_article(df, query) 

        # 2. ì•„í‹°í´ ì¡´ì¬ ì—¬ë¶€ íŒŒì•…
        if recommend_article is None: # ì¶”ì²œ ì•„í‹°í´ ì¡´ì¬ X
            if df.empty: # í›„ë³´ ê¸°ì‚¬ ëª©ë¡ì´ ë¹ˆ ê²½ìš°
                print("âš ï¸ í›„ë³´ ê¸°ì‚¬ ëª©ë¡ì´ ë¹„ì–´ ì¶”ì²œí•  ìˆ˜ ìˆëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            else: 
                fail += 1
                continue # ì¬ì¶”ì²œ
        else: # ì¶”ì²œ ì•„í‹°í´ ì¡´ì¬ O
            url = recommend_article["Link"] 
            domain = recommend_article["Domain"]
            title = recommend_article["Title"]

        # 3. ë³¸ë¬¸ ì¶”ì¶œ
        article_body = get_article_body(url, domain)
        
        if ( not article_body or len([s for s in article_body.split(".") if s.strip()]) <= 5 ): # ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ 5ë¬¸ì¥ ì´í•˜ì¸ ê²½ìš°
            print(f"ğŸ” ë³¸ë¬¸ì´ ì—†ëŠ” ì•„í‹°í´ (ë˜ëŠ” ë³¸ë¬¸ì´ 5ë¬¸ì¥ ì´í•˜)ì´ë¯€ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            df.drop(df[df["Link"] == url].index, inplace=True) # df(í›„ë³´ ê¸°ì‚¬ ëª©ë¡)ì—ì„œ ì‚­ì œ
            fail += 1
            continue   # ì¬ì¶”ì²œ
        else: # ì¶”ì¶œ ì„±ê³µ
            return { "Title": title, "URL": url, "Body": article_body}
    
    # ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
    return None
    


# ì¶”ì²œ ì•„í‹°í´ ê²°ì •#
def find_recommend_article(df_google:pd.DataFrame, query:str="") -> Dict:
    fail = 0
    # DataFrameì˜ ê° í•­ëª© ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜í•´ ì €ì¥
    article_descriptions = df_google["Description"].tolist()
    article_indices = df_google.index.tolist()

    while fail < 3:
        try:
            # Open API í˜¸ì¶œ
                # í† í° ì´ˆê³¼ ì—ëŸ¬ ë°œìƒí•´ì„œ, title ì •ë³´ëŠ” ì œì™¸í•¨!
            system_prompt = f"""
            # ì§€ì‹œë¬¸
                - ë‹¹ì‹ ì€ ì•„í‹°í´ì˜ í‚¤ì›Œë“œì¸ queryì™€ ì•„í‹°í´ì˜ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì•„í‹°í´ì„ ì¶”ì²œí•˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì˜ ì—­í• ì„ í•œë‹¤.
            # ì¶”ì²œ ì¡°ê±´
                1. queryì—ì„œ ë‹¤ë£¨ëŠ” ì£¼ì œì™€ ê´€ë ¨ì´ ìˆëŠ” ì•„í‹°í´ì„ ì„ íƒí•˜ì„¸ìš”.
                2. ì œëª©ê³¼ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„í‹°í´ì˜ ì í•©ì„±ì„ íŒë‹¨í•˜ì„¸ìš”.
                3. ë‹¨ìˆœ ë‰´ìŠ¤ ë³´ë„, ê´‘ê³ ì„± ë‚´ìš©, ë˜ëŠ” ì¤‘ë³µëœ ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”.
                4. ì§€ì‹ì ì¸ ì„¤ëª… ë˜ëŠ” í•™ìŠµì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.

            # ì¶œë ¥ í˜•ì‹
              - index ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”
                ì˜ˆ:
                ```
                    6
                ```
            """
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": (
                            f"query: {query}\n\n"
                            "ì•„í‹°í´ ëª©ë¡ (index í¬í•¨):\n"
                            + "\n".join(
                                f"{i}. [Index: {idx}]  ì„¤ëª…: {description}\n  "
                                for i, (idx, description) in enumerate(
                                    zip(
                                        article_indices,
                                        article_descriptions,
                                    )
                                )
                            )
                        ),
                    },
                ],
                temperature=0,
                max_tokens=2048,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0,
            )


            # GPTì˜ ì‘ë‹µ ë¶„ì„ 
            content = response["choices"][0]["message"]["content"]

            # ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜ ì‹œë„
            try:
                recommended_index = int(content)  # ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜
            except ValueError:
                recommended_index  = 0  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •

            # indexê°€ DataFrameì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if recommended_index not in df_google.index:
                print(f"âš ï¸ ì¶”ì²œëœ index({recommended_index})ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                break

            # í•´ë‹¹ indexë¡œ í–‰(í•´ë‹¹ ê¸°ì‚¬ ì •ë³´) ë°˜í™˜
            recommended_row = df_google.loc[recommended_index]
            recommended_article = {
                "Link": recommended_row["Link"],
                "Domain": recommended_row["Domain"],
                "Title": recommended_row["Title"]
            }
            return recommended_article
        except openai.error.RateLimitError:
            print("ğŸ” Rate limit reached. Retrying in 40 seconds...")
            time.sleep(40)  # 40ì´ˆ ì§€ì—° í›„ ì¬ì‹œë„
            fail += 3
            continue

    return None # ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
    

def get_article_body(url:str, domain:str) -> str:
    # ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì´íŠ¸ë³„ íƒœê·¸ ì •ë³´#
    SITE_CLASS_MAPPING = {
        # "brunch.co.kr": [{"tag": "div", "class": "wrap_body"}],
        "bbc.com": [{"tag": "main", "class": "bbc-fa0wmp"}],
        "khan.co.kr": [{"tag": "div", "class": "art_body"}],
        "hani.co.kr": [{"tag": "div", "class": "article-text"}],
        "ytn.co.kr": [{"tag": "div", "class": "vodtext"}],
        "sisain.co.kr": [{"tag": "div", "class": "article-body"}],
        "news.sbs.co.kr": [{"tag": "div", "class": "main_text"}],
        "h21.hani.co.kr": [{"tag": "div", "class": "arti-txt"}],
        "ohmynews.com": [
            {"tag": "article", "class": "article_body at_contents article_view"},
            {"tag": "div", "class": "news_body"},
        ],
        # ê³µë°± ì£¼ì˜ # ì¶”ê°€ ë„ë©”ì¸ ë° íƒœê·¸/í´ë˜ìŠ¤ ë§¤í•‘
        # í•œêµ­ì¼ë³´, mbcnewsëŠ” ì œì™¸
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    }

    try:
        # URLì—ì„œ HTML (ìš”ì²­)ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, headers=headers)
        response.raise_for_status() # ì˜¤ë¥˜ ë°œìƒí•˜ë©´, ì˜ˆì™¸ ë°œìƒ
        soup = BeautifulSoup(response.text, "html.parser")

        # ë„ë©”ì¸ì´ ì œê³µëœ ê²½ìš° SITE_CLASS_MAPPINGì—ì„œ ì²˜ë¦¬
        site_info = SITE_CLASS_MAPPING.get(domain)
        if not site_info: # í•´ë‹¹ ë„ë©”ì¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° 
            return None

        # ëª¨ë“  ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒí•˜ë©° íƒœê·¸/í´ë˜ìŠ¤ ì²˜ë¦¬
        for mapping in site_info:
            tag_name = mapping.get("tag")
            class_name = mapping.get("class")
            main_body = soup.find(tag_name, class_=class_name)
            if main_body:
                # íƒœê·¸ ë‚´ë¶€ì— p, h1 ë“±ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
                text_elements = main_body.find_all(["h1", "h2", "h3", "h4", "p", "li"])
                # <p> íƒœê·¸ ê°œìˆ˜ í™•ì¸ (2ê°œ ì´í•˜ì´ë©´ ë³¸ë¬¸ì´ ë¶€ì¡±í•˜ë‹¤ê³  ê°„ì£¼)

                paragraph_count = len(main_body.find_all("p"))
                if paragraph_count <= 2:
                    return main_body.get_text(strip=True)

                if text_elements:
                    return "\n".join(
                        [element.get_text(strip=True) for element in text_elements]
                    )
                else:
                    return main_body.get_text(strip=True)
        # í•´ë‹¹ ë„ë©”ì¸ì— ë§¤í•‘ëœ íƒœê·¸ì™€ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° 
        return None 
    except requests.exceptions.RequestException as e: # HTTP ìš”ì²­ ì˜¤ë¥˜
        return None
    except Exception as e: # ë³¸ë¬¸ ì¶”ì¶œì¤‘ ì˜¤ë¥˜
        return None

