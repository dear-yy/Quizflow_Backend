import os
import sys
import openai
import time
import json
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup
from typing import List, Dict, Tuple
from django.conf import settings
from django.contrib.auth.models import User
from quiz_room.models import Article

# Django í”„ë¡œì íŠ¸ ì ˆëŒ€ ê²½ë¡œë¡œ ì¶”ê°€
# ë‘ ë‹¨ê³„ ìœ„ë¡œ ì˜¬ë¼ê°€ì„œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬(myquiz/)ì— ì ‘ê·¼
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))  # __file__ : í˜„ì¬ ê²½ë¡œ
# DJANGO_SETTINGS_MODULE í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ Django ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myquiz.settings')

# í‚¤ ì„¤ì • 
openai.api_key = settings.OPENAI_API_KEY 
GOOGLE_SEARCH_ENGINE_ID  = settings.GOOGLE_SEARCH_ENGINE_ID
GOOGLE_API_KEY = settings.GOOGLE_API_KEY


'''
get_keywords_from_feedback
    - extract_keywords
'''

def get_keywords_from_feedback(recent_user_feedback:str, user_feedback_list:list, keyword_list:list) -> Tuple[List[str], str]:
    # 1. ìƒˆë¡œìš´ í‚¤ì›Œë“œ ì¶”ì¶œ
    new_keyword_list = extract_keywords(False, user_feedback_list)

    # 2. ì¶”ì¶œ í‚¤ì›Œë“œ kewords_listì— ì—°ê²°(ëˆ„ì  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ -> ê²€ìƒ‰ì¿¼ë¦¬)
    search_query = " ".join(new_keyword_list)

    return (new_keyword_list, search_query)

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(retry:bool, user_feedback_list:str, max_keywords:int=3) -> List:
    fail_cnt = 0  # ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
    recent_user_feedback = user_feedback_list[-1]
    while fail_cnt < 3:
        try:
            # SEO ìµœì í™”ëœ í‚¤ì›Œë“œë€ -> ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì‚¬ëŒë“¤ì´ ìì£¼ ê²€ìƒ‰í•˜ëŠ” ë‹¨ì–´( ë§ì€ ì‚¬ëŒë“¤ì´ ê²€ìƒ‰í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ì€ í‚¤ì›Œë“œ)
            system_prompt = f"""
                ë‹¹ì‹ ì˜ ì—­í• ì€ **ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ SEO(ê²€ìƒ‰ ì—”ì§„ ìµœì í™”)ì— ìµœì í™”ëœ í‚¤ì›Œë“œ 3ê°œë¥¼ ìƒì„±**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

                # 1 í‚¤ì›Œë“œ ìƒì„± ì „ëµ:
                    - ê°€ì¥ ìµœê·¼ í”¼ë“œë°±({recent_user_feedback})ì„ **ì¤‘ì‹¬ìœ¼ë¡œ** ì‚¼ì•„,
                      ê·¸ ì˜ë¯¸ë¥¼ ë” êµ¬ì²´í™”í•˜ê³  í™•ì¥í•  ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
                    - ê³¼ê±° í”¼ë“œë°± ë¦¬ìŠ¤íŠ¸({user_feedback_list})ì˜ ìš”ì†Œë“¤ì„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬
                      **ìµœê·¼ í”¼ë“œë°±ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê²°**í•˜ê±°ë‚˜ **ì‹¬í™”ëœ ë°©í–¥**ìœ¼ë¡œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
                      ì˜ˆ: ê³¼ê±° í”¼ë“œë°±="í™˜ê²½ ì˜¤ì—¼", ê°€ì¥ ìµœê·¼ í”¼ë“œë°±="ì‚°ë¶ˆ" â†’ ìƒì„± í‚¤ì›Œë“œ: `"ì‚°ë¶ˆ ì›ì¸"`, `"í™˜ê²½ ì˜¤ì—¼ í”¼í•´ ì‚¬ë¡€"`, `"ê¸°í›„ë³€í™” ì‚°ë¶ˆ"`
                    - {retry}ê°€ Trueì´ë©´, ê¸°ì¡´ í‚¤ì›Œë“œë¡œ ì•„í‹°í´ ê²€ìƒ‰ì— ì‹¤íŒ¨í•œ ìƒí™©ì´ë¯€ë¡œ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì¡´ì¬í• ë§Œí•œ ë” í¬ê´„ì ì¸ í‚¤ì›Œë“œë¡œ ìƒì„±í•˜ì„¸ìš”. 

                # 2 í‚¤ì›Œë“œ ì¶”ì¶œ ê·œì¹™:
                    - ìƒì„±í•˜ëŠ” í‚¤ì›Œë“œëŠ” **ê²€ìƒ‰ ì—”ì§„ì—ì„œ ìì£¼ ê²€ìƒ‰ë  ê°€ëŠ¥ì„±ì´ ë†’ì€** **ëª…ì‚¬ ì¤‘ì‹¬ì˜ êµ¬ì²´ì  í‘œí˜„**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
                    - ëª…í™•í•˜ê³  ê²€ìƒ‰ ì¹œí™”ì ì¸ ë‹¨ì–´ë¡œ êµ¬ì„±í•˜ì„¸ìš”.
                    - ê°€ëŠ¥í•œ í•œ **ì‚¬ìš©ìì˜ ìµœì‹  í”¼ë“œë°±({recent_user_feedback})**ì„ ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ ë°˜ì˜í•˜ì„¸ìš”.
  
                # 3 í”¼ë“œë°± ìœ íš¨ì„± íŒë‹¨ ë° ì²˜ë¦¬:
                    - ë§Œì•½ {recent_user_feedback}ì´ ë¬´ì˜ë¯¸í•œ ì…ë ¥ (ì˜ˆ: "ã…ã…‡ë‹ˆëŸ¬", "ëª¨ë¥´ê² ë‹¤", "ì•„ë¬´ê±°ë‚˜")ì¼ ê²½ìš°:
                        1. ì „ì²´ í”¼ë“œë°± ë¦¬ìŠ¤íŠ¸ {user_feedback_list}ë¥¼ **ìµœì‹ ìˆœ(ì¸ë±ìŠ¤ê°€ í° ìˆœì„œëŒ€ë¡œ)**ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
                        2. ë§Œì•½ {user_feedback_list}ê°€ ë¹„ì–´ìˆê±°ë‚˜ ëª¨ë“  ìš”ì†Œê°€ ë¬´ì˜ë¯¸í•œ ë¬¸ìì—´ì¸ ê²½ìš°:
                    - ["ì—­ì‚¬", "ì² í•™", "ê³¼í•™", "ì˜ˆìˆ ", "ê¸°ìˆ ", "ë¬¸í™”", "ê±´ê°•"] ì¤‘ í•˜ë‚˜ì˜ ì£¼ì œë¥¼ ì„ì˜ë¡œ ì„ íƒí•˜ì—¬ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

                # 4 ì¶œë ¥ í˜•ì‹:
                    - ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
                        ì˜ˆì‹œ: {{"k1":"í‚¤ì›Œë“œ1", "k2":"í‚¤ì›Œë“œ2", "k3":"í‚¤ì›Œë“œ3"}}
            """

            # Open API í˜¸ì¶œ
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": "system ì§€ì¹¨ì— ë”°ë¼ ìµœê·¼ ì‚¬ìš©ì í”¼ë“œë°±ì„ ì¤‘ì‹¬ìœ¼ë¡œ SEO ìµœì í™” í‚¤ì›Œë“œ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."},
                ],
                temperature=0,
                max_tokens=2048,
                top_p=1, # ì¶”ì¶œ ë‹¨ì–´ì˜ ê´€ë ¨ì„± ì œì–´
                frequency_penalty=0, # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë¥¼ ì œì–´(ê³ ë ¤)
                presence_penalty=0, # ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì œì–´(ê³ ë ¤)
            )

            # GPTì˜ ì‘ë‹µ ë¶„ì„ 
            keywords = response["choices"][0]["message"]["content"]

            # (JSON -> ë”•ì…”ë„ˆë¦¬) ë³€í™˜
            keywords = keywords.replace("'", "\"") # JSON í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
            try:
                keywords_dict = json.loads(keywords)
                keywords_list = list(keywords_dict.values()) # (ë”•ì…”ë„ˆë¦¬ value -> ë¦¬ìŠ¤íŠ¸) ë³€í™˜
            except json.JSONDecodeError as e:
                fail_cnt += 1
                print(f"âš ï¸ [extract_keywords] JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì‘ë‹µ ë‚´ìš©: {response['choices'][0]['message']['content']}")
                continue  # ì¬ì‹œë„
            return keywords_list # ì •ìƒ ì¶”ì¶œ
        except openai.error.RateLimitError:
            fail_cnt += 1
            print("ğŸ” Rate limitì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. 40ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
            time.sleep(40)
        except Exception as e:
            fail_cnt += 1
            print(f"ğŸ” Error during OpenAI API call: {e}")

    print("âš ï¸ 3ë²ˆ ì´ìƒì˜ ì‹¤íŒ¨ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    return []  # 3ë²ˆ ì´ìƒ ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜






'''
    select_article
        - Google_API
        - process_recommend_article
            - find_recommend_article
            - get_article_body
            - Google_API(ì¬ìš”ì²­ ì‹œ)
        
'''
def select_article(user:User, query:str, user_feedback_list:list) -> Dict: # (ì‚¬ìš©ì ê°ì²´, ëˆ„ì  í‚¤ì›Œë“œ, ëˆ„ì  í”¼ë“œë°±)
    fail = 0                    
    retry_extracted_keywords = None # í‚¤ì›Œë“œ ì¬ì¶”ì¶œ ì‹œ
    num_results_per_site = 3    # ì‚¬ì´íŠ¸ë‹¹ ê²°ê³¼ ê°œìˆ˜
    sites = [                   # ê²€ìƒ‰ ê°€ëŠ¥ ì‚¬ì´íŠ¸ ëª©ë¡ 
        # "brunch.co.kr",
        "bbc.com",
        "khan.co.kr",
        "hani.co.kr",
        "ytn.co.kr",
        "sisain.co.kr",
        "news.sbs.co.kr",
        "h21.hani.co.kr",
        "ohmynews.com",
        # ì¶”ê°€í•˜ê¸°
    ]
    recommend_article_title, recommend_article_body, recommend_article_url, recommend_article_reason = "ì‹¤íŒ¨", "ì‹¤íŒ¨", "ì‹¤íŒ¨", "ì‹¤íŒ¨"

    # 1. í›„ë³´ ê¸°ì‚¬ ëª©ë¡ ì„œì¹˜ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê¸°ë°˜ ì¿¼ë¦¬ë¡œ)
    df = Google_API(user, query, num_results_per_site, sites)  # í›„ë³´ ê¸°ì‚¬ ëª©ë¡
    time.sleep(10)  # ìƒì„± í† í° ì œí•œ ì—ëŸ¬ ì˜ˆë°©
    
    # 2. ì¶”ì²œ ì•„í‹°í´ ê²°ì • 
    while fail<3 :
        # ì•„í‹°í´ ì¶”ì²œ gpt ìš”ì²­
        info_for_the_article = process_recommend_article(df, user_feedback_list) 

        if info_for_the_article is None: # ì•„í‹°í´ ì¶”ì²œ ì‹¤íŒ¨
            fail += 1
            # í‚¤ì›Œë“œ ì¬ì¶”ì¶œ -> ê²€ìƒ‰ ì¿¼ë¦¬ ì¬êµ¬ì„±
            retry_extracted_keywords = extract_keywords(True, user_feedback_list, max_keywords=3)
            if retry_extracted_keywords:
                query = " ".join(retry_extracted_keywords) # ê²€ìƒ‰ ì¿¼ë¦¬ ì¬êµ¬ì„±
                df = Google_API(user, query, num_results_per_site=5, sites=sites) # # í›„ë³´ ê¸°ì‚¬ ëª©ë¡ ì¬êµ¬ì„±
        else: # ì•„í‹°í´ ì¶”ì²œ ì„±ê³µ
            # Title, URL ë° Body ì¶”ì¶œ
            recommend_article_title = info_for_the_article["Title"]
            recommend_article_body = info_for_the_article["Body"]
            recommend_article_url = info_for_the_article["URL"]
            recommend_article_reason = info_for_the_article["Reason"]
            print("âœ… ì¶”ì²œ ì•„í‹°í´ URL:", recommend_article_url)
            break  # ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
    
    # 3. ìµœì¢… ì¶”ì²œ ì•„í‹°í´ ì •ë³´ ë°˜í™˜
    return  {
        "title": recommend_article_title,
        "body": recommend_article_body, 
        "url": recommend_article_url, 
        "reason": recommend_article_reason, 
        "retry_extracted_keywords": retry_extracted_keywords # í‚¤ì›Œë“œ ì¬ì¶”ì¶œì‹œ, DBì— ë°˜ì˜í•˜ê¸° ìœ„í•¨ 
    }



# í›„ë³´ ê¸°ì‚¬ ì •ë³´ ("Title", "Description", "Link", "Domain") í˜•ì‹
def Google_API(user:User, query:str, num_results_per_site:int, sites:list[str]) -> pd.DataFrame: # DataFrame:2ì°¨ì› í–‰&ì—´ ë°ì´í„° êµ¬ì¡°
    # í›„ë³´ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    df_google_list = [] 
    # ì‚¬ìš©ì ê³¼ê±° ì•„í‹°í´ ë‚´ì—­ 
    past_articles = set(Article.objects.filter(user=user).order_by('-timestamp')[:100].values_list('url', flat=True)) # userì˜ ê³¼ê±° ì•„í‹°í´ì„ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ í›„, ìƒìœ„ 100ê°œ ë°˜í™˜(ì¤‘ë³µì€ ì‚­ì œ)

    # ì‚¬ì´íŠ¸ ë³„ í›„ë³´ ê¸°ì‚¬
    for site in sites: 
        # 1. Google Custom Search APIì— ëŒ€í•œ ìš”ì²­ URL êµ¬ì„±
        site_query = f"site:{site} {query}"     # ê° ì‚¬ì´íŠ¸ ë³„ ê²€ìƒ‰ì–´ êµ¬ì„±
        collected_results_cnt = 0               # ìˆ˜ì§‘í•œ ê²°ê³¼ ê°œìˆ˜
        start_index = 1                         # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ íƒìƒ‰ ì‹œì‘ ìœ„ì¹˜
        num = 5                                 # í•œ ë²ˆì— ë°˜í™˜í•  ê²°ê³¼ì˜ ê°œìˆ˜
        url = f"https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={GOOGLE_SEARCH_ENGINE_ID}&q={site_query}&start={start_index}&num={num}" 
            
        while collected_results_cnt < num_results_per_site: # ì •í•´ì§„ ê°œìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€    
            try:
                # 2. URLë¡œ HTTP GET ìš”ì²­
                response = requests.get(url)

                # 3. ìš”ì²­ ì„±ê³µ ì—¬ë¶€ì— ë”°ë¥¸ ì²˜ë¦¬ 
                if response.status_code != 200: # ìš”ì²­ ì‹¤íŒ¨
                    print(f"âš ï¸ {site}ì— ëŒ€í•œ HTTP GET ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, Message: {response.text}")
                    break 
                else: # ìš”ì²­ ì„±ê³µ 
                    data = response.json() # ì‘ë‹µ ë°ì´í„° -> JSON í˜•ì‹
                    search_items = data.get("items") # ê²€ìƒ‰ ê²°ê³¼ í•­ëª©(ê¸°ì‚¬)ë“¤ ê°€ì ¸ì˜¤ê¸°
                    if not search_items: # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´
                        print(f"ğŸ” '{site}'ì— {site_query}ì— ëŒ€í•œ ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break 

                # 4. ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                for search_item in search_items: 
                    # ê²°ê³¼ ê°œìˆ˜ ì²´í¬
                    if collected_results_cnt >= num_results_per_site: # ê²°ê³¼ ê°œìˆ˜ ë„ë‹¬
                        break # for ë£¨í”„ ì¢…ë£Œ

                    # ì •ë³´ ì¶”ì¶œ
                    link, title, description = search_item.get("link"), search_item.get("title"), search_item.get("snippet")

                    # ì‚¬ìš©ì ê³¼ê±° ì•„í‹°í´ê³¼ ì¤‘ë³µ ë°©ì§€
                    if link in past_articles:
                        continue  

                    # ì¶”ê°€ ì¡°ê±´: m.khan.co.kr ì²˜ë¦¬
                    if site == "khan.co.kr" and "m.khan.co.kr" in link:
                        link = link.replace("m.khan.co.kr", "khan.co.kr") # (ëª¨ë°”ì¼ ë§í¬ -> ì¼ë°˜ ì›¹ì‚¬ì´íŠ¸ ë§í¬)ë³€í™˜

                    # ê¸°ì‚¬ ì •ë³´ì— ëŒ€í•œ DataFrame ìƒì„± í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    df_google_list.append( pd.DataFrame(
                            [[title, description, link, site]], # ë‹¤ìŒ í–‰ ì¶”ê°€(í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ 1ê°œì˜ í–‰ êµ¬ì„±)
                            columns=["Title", "Description", "Link", "Domain"], # ì—´ ì •ë³´
                    )   )
                    collected_results_cnt += 1  # ìˆ˜ì§‘ ê²°ê³¼ ê°œìˆ˜ ì¦ê°€
            except Exception as e:
                print(f"âš ï¸ Error occurred for site {site}: {e}")
                break

    # 5. ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
    if df_google_list: 
        df_google = pd.concat(df_google_list, ignore_index=True)  # ignore_index=True: ìƒˆë¡œìš´ ì—°ì†ì ì¸ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬
    else:  # df_google_listê°€ ë¹„ì–´ìˆë‹¤ë©´
        df_google = pd.DataFrame(columns=["Title", "Description", "Link", "Domain"]) # ë¹ˆ DataFrame 

    return df_google
   



# gpt ê¸°ë°˜ ì•„í‹°í´ ì¶”ì²œ í›„, body ì •ë³´ ì¶”ì¶œí•˜ì—¬ ì¶”ì²œ ê¸°ì‚¬ ì •ë³´ ë°˜í™˜í™˜ 
    # ì¶”ì²œëœ ì•„í‹°í´ì´ ì—†ê±°ë‚˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì„ ê²½ìš°, í•´ë‹¹ ë°ì´í„°ë¥¼ ì‚­ì œ í›„ ì¬ì¶”ì²œ
def process_recommend_article(df:pd.DataFrame=None, user_feedback:str="") -> Dict:
    # ì´ˆê¸°í™”
    fail = 0

    # ì¶”ì²œ ì•„í‹°í´ ì„ ì • í›„, body ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤
    while fail < 3:
        # 1. ì¶”ì²œ ì•„í‹°í´ ë°˜í™˜
        recommend_article, reason = find_recommend_article(df, user_feedback) 

        # 2. ì•„í‹°í´ ì¡´ì¬ ì—¬ë¶€ íŒŒì•…
        if recommend_article is None: # ì¶”ì²œ ì•„í‹°í´ ì¡´ì¬ X
            if df.empty: # í›„ë³´ ê¸°ì‚¬ ëª©ë¡ì´ ë¹ˆ ê²½ìš°
                print("âš ï¸ í›„ë³´ ê¸°ì‚¬ ëª©ë¡ì´ ë¹„ì–´ ì¶”ì²œí•  ìˆ˜ ìˆëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            else: 
                fail += 1
                continue # ì¬ì¶”ì²œ
        else: # ì¶”ì²œ ì•„í‹°í´ ì¡´ì¬ O
            url = recommend_article["Link"] 
            domain = recommend_article["Domain"]
            title = recommend_article["Title"]

        # 3. ë³¸ë¬¸ ì¶”ì¶œ
        article_body = get_article_body(url, domain)
        
        if ( not article_body or len([s for s in article_body.split(".") if s.strip()]) <= 5 ): # ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ 5ë¬¸ì¥ ì´í•˜ì¸ ê²½ìš°
            print(f"ğŸ” ë³¸ë¬¸ì´ ì—†ëŠ” ì•„í‹°í´ (ë˜ëŠ” ë³¸ë¬¸ì´ 5ë¬¸ì¥ ì´í•˜)ì´ë¯€ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            df.drop(df[df["Link"] == url].index, inplace=True) # df(í›„ë³´ ê¸°ì‚¬ ëª©ë¡)ì—ì„œ ì‚­ì œ
            fail += 1
            continue   # ì¬ì¶”ì²œ
        else: # ì¶”ì¶œ ì„±ê³µ
            return { "Title": title, "URL": url, "Body": article_body, "Reason": reason }
    
    # ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
    return None
    


# ì¶”ì²œ ì•„í‹°í´ ê²°ì •#
def find_recommend_article(df_google:pd.DataFrame, user_feedback_list:list) -> Tuple[Dict, str]:
    fail = 0
    # DataFrameì˜ ê° í•­ëª© ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜í•´ ì €ì¥
    article_descriptions = df_google["Description"].tolist()
    article_indices = df_google.index.tolist()
        

    while fail < 3:  
        try:
            # Open API í˜¸ì¶œ         # í† í° ì´ˆê³¼ ì—ëŸ¬ ë°œìƒí•´ì„œ, title ì •ë³´ëŠ” í”„ë¡¬í”„íŠ¸ì—ì„œ ì œì™¸í•¨!
            system_prompt = f"""
            # ì§€ì‹œë¬¸
                - ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ í”¼ë“œë°±ê³¼ ì•„í‹°í´ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ
                  ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì•„í‹°í´ì„ ì¶”ì²œí•˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì˜ ì—­í• ì„ í•œë‹¤.
            # ì¶”ì²œ ì¡°ê±´
                1. ìµœì‹  í”¼ë“œë°±(ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¸ë±ìŠ¤ê°€ ë†’ì€ ìˆœì„œ)ì„ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.
                2. ìµœì‹  í”¼ë“œë°±ì´ ë‹¤ë£¨ëŠ” ì£¼ì œì™€ ê°€ì¥ ê´€ë ¨ì´ ìˆëŠ” ì•„í‹°í´ì„ ì„ íƒí•˜ì„¸ìš”.
                3. ì œëª©ê³¼ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„í‹°í´ì˜ ì í•©ì„±ì„ íŒë‹¨í•˜ì„¸ìš”.
                4. ë‹¨ìˆœ ë‰´ìŠ¤ ë³´ë„, ê´‘ê³ ì„± ë‚´ìš©, ë˜ëŠ” ì¤‘ë³µëœ ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”.
                5. ì§€ì‹ì ì¸ ì„¤ëª… ë˜ëŠ” í•™ìŠµì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.

            # ì¶œë ¥ í˜•ì‹
              - ë‹µë³€ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”. 
              - ë”•ì…”ë„ˆë¦¬ key ì´ë¦„ì€ "index"ì™€ "reason"ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.
              - "reason" keyì˜ valueì— í•´ë‹¹í•˜ëŠ” ë¬¸ìì—´ ë‚´ë¶€ì—ì„œ ì‘ì€ë”°ì˜´í‘œ(')ì™€ í°ë”°ì˜´í‘œ(")ê°€ ë“±ì¥í•˜ì§€ ì•Šë„ë¡ ë¬¸ìì—´ì„ êµ¬ì„±í•˜ì„¸ìš”.
              - "reason" keyì˜ valueëŠ” 2ë¬¸ì¥ ì •ë„ì˜ í•œêµ­ì–´ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
                ì˜ˆ:
                ```
                    {{"index": "ì¶”ì²œëœ ì•„í‹°í´ì˜ ê³ ìœ  index", "reason": "ì™œ ì´ ì•„í‹°í´ì´ ì í•©í•œì§€ ê°„ë‹¨íˆ ì„¤ëª…" }}
                ```
            """
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    { "role": "system", "content": system_prompt,},
                    {   "role": "user",
                        "content": (
                            f"ì‚¬ìš©ì í”¼ë“œë°±: {user_feedback_list}\n\n"
                            "ì•„í‹°í´ ëª©ë¡ (index í¬í•¨):\n"
                            + "\n".join(
                                f"{i}. [Index: {idx}]  ì•„í‹°í´ ì„¤ëª…: {description}\n  "
                                for i, (idx, description) in enumerate(
                                    zip(article_indices, article_descriptions,)
                                )
                            )
                        ),
                    },
                ],
                temperature=0,
                max_tokens=2048,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0,
            )

            # GPTì˜ ì‘ë‹µ ë¶„ì„ 
            content = response["choices"][0]["message"]["content"]
            # (JSON -> ë”•ì…”ë„ˆë¦¬) ë³€í™˜ ì‘ì—…
            content = re.sub(r'"reason":\s*"([^"]*?)"', escape_inner_quotes, content)
            try:
                content_dict = json.loads(content)
            except json.JSONDecodeError as e:
                print(f"âš ï¸ [find_recommend_article] JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì‘ë‹µ ë‚´ìš©: {response['choices'][0]['message']['content']}")
                fail += 1 
                continue

            # content ì¡´ì¬ ê²€ì¦
            if not isinstance(content_dict["index"], int): 
                break
            elif not isinstance(content_dict["reason"], str) or not content_dict["reason"]: # íƒ€ì…ê³¼ ì¡´ì¬ ì—¬ë¶€
                break

            # ì •ìˆ˜ index ì €ì¥ 
            recommended_index = int(content_dict["index"])  
            # indexê°€ DataFrameì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if recommended_index not in df_google.index:
                print(f"âš ï¸ ì¶”ì²œëœ index({recommended_index})ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                break

            # í•´ë‹¹ indexë¡œ í–‰(í•´ë‹¹ ê¸°ì‚¬ ì •ë³´) ë°˜í™˜
            recommended_row = df_google.loc[recommended_index]
            recommended_article = {
                "Link": recommended_row["Link"],
                "Domain": recommended_row["Domain"],
                "Title": recommended_row["Title"]
            }
            reason = content_dict["reason"]
            return (recommended_article, reason) # ê²°ê³¼ DataFrame í˜•íƒœë¡œ ë°˜í™˜

        except openai.error.RateLimitError:
            print("ğŸ” Rate limit reached. Retrying in 40 seconds...")
            fail += 1
            time.sleep(40)  # 40ì´ˆ ì§€ì—° í›„ ì¬ì‹œë„
            continue 

    return (None, "") # ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ 
    


def get_article_body(url:str, domain:str) -> str:
    # ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì´íŠ¸ë³„ íƒœê·¸ ì •ë³´#
    SITE_CLASS_MAPPING = {
        # "brunch.co.kr": [{"tag": "div", "class": "wrap_body"}],
        "bbc.com": [{"tag": "main", "class": "bbc-fa0wmp"}],
        "khan.co.kr": [{"tag": "div", "class": "art_body"}],
        "hani.co.kr": [{"tag": "div", "class": "article-text"}],
        "ytn.co.kr": [{"tag": "div", "class": "vodtext"}],
        "sisain.co.kr": [{"tag": "div", "class": "article-body"}],
        "news.sbs.co.kr": [{"tag": "div", "class": "main_text"}],
        "h21.hani.co.kr": [{"tag": "div", "class": "arti-txt"}],
        "ohmynews.com": [
            {"tag": "article", "class": "article_body at_contents article_view"},
            {"tag": "div", "class": "news_body"},
        ],
        # ê³µë°± ì£¼ì˜ # ì¶”ê°€ ë„ë©”ì¸ ë° íƒœê·¸/í´ë˜ìŠ¤ ë§¤í•‘
        # í•œêµ­ì¼ë³´, mbcnewsëŠ” ì œì™¸
    }

    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"}

    try:
        # URLì—ì„œ HTML (ìš”ì²­)ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, headers=headers)
        response.raise_for_status() # ì˜¤ë¥˜ ë°œìƒí•˜ë©´, ì˜ˆì™¸ ë°œìƒ
        soup = BeautifulSoup(response.text, "html.parser")

        # ë„ë©”ì¸ì´ ì œê³µëœ ê²½ìš° SITE_CLASS_MAPPINGì—ì„œ ì²˜ë¦¬
        site_info = SITE_CLASS_MAPPING.get(domain)
        if not site_info: # í•´ë‹¹ ë„ë©”ì¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° 
            return None

        # ëª¨ë“  ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒí•˜ë©° íƒœê·¸/í´ë˜ìŠ¤ ì²˜ë¦¬
        for mapping in site_info:
            tag_name = mapping.get("tag")
            class_name = mapping.get("class")
            main_body = soup.find(tag_name, class_=class_name)
            if main_body:
                # íƒœê·¸ ë‚´ë¶€ì— p, h1 ë“±ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
                text_elements = main_body.find_all(["h1", "h2", "h3", "h4", "p", "li"])
                # <p> íƒœê·¸ ê°œìˆ˜ í™•ì¸ (2ê°œ ì´í•˜ì´ë©´ ë³¸ë¬¸ì´ ë¶€ì¡±í•˜ë‹¤ê³  ê°„ì£¼)

                paragraph_count = len(main_body.find_all("p"))
                if paragraph_count <= 2:
                    return main_body.get_text(strip=True)

                if text_elements:
                    return "\n".join(
                        [element.get_text(strip=True) for element in text_elements]
                    )
                else:
                    return main_body.get_text(strip=True)
        # í•´ë‹¹ ë„ë©”ì¸ì— ë§¤í•‘ëœ íƒœê·¸ì™€ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° 
        return None 
    except requests.exceptions.RequestException as e: # HTTP ìš”ì²­ ì˜¤ë¥˜
        return None
    except Exception as e: # ë³¸ë¬¸ ì¶”ì¶œì¤‘ ì˜¤ë¥˜
        return None



# ì •ê·œì‹: "key": "value"
# keyë‚˜ ì „ì²´ êµ¬ì¡°ì— ìˆëŠ” í°ë”°ì˜´í‘œëŠ” ê±´ë“œë¦¬ë©´ ì•ˆ ë˜ë¯€ë¡œ value ë‚´ë¶€ë§Œ ë°”ê¿”ì•¼ í•¨
def escape_inner_quotes(match):
    inner = match.group(1)
    escaped = re.sub(r'"', r'\\"', inner)  # í°ë”°ì˜´í‘œ escape
    return f'"reason": "{escaped}"'