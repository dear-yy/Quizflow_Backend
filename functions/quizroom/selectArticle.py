import os
import sys
import openai
import time
import json
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup
from typing import List, Dict, Tuple
from django.conf import settings
from django.contrib.auth.models import User
from quiz_room.models import Article

# Django í”„ë¡œì íŠ¸ ì ˆëŒ€ ê²½ë¡œë¡œ ì¶”ê°€
# ë‘ ë‹¨ê³„ ìœ„ë¡œ ì˜¬ë¼ê°€ì„œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬(myquiz/)ì— ì ‘ê·¼
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))  # __file__ : í˜„ì¬ ê²½ë¡œ
# DJANGO_SETTINGS_MODULE í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ Django ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myquiz.settings')

# í‚¤ ì„¤ì • 
openai.api_key = settings.OPENAI_API_KEY 
GOOGLE_SEARCH_ENGINE_ID  = settings.GOOGLE_SEARCH_ENGINE_ID
GOOGLE_API_KEY = settings.GOOGLE_API_KEY


'''
get_keywords_from_feedback
    - extract_keywords
'''

def get_keywords_from_feedback(user_feedback:str, user_feedback_list:list, keyword_list:list) -> Tuple[List[str], str]:
    # í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì‚¬ ê²€ìƒ‰ì–´ ì„¤ì •)
    query = " ".join(keyword_list)  # ê²€ìƒ‰ì–´ # user_feedbackì´ ë¹ˆ ê²½ìš°, ëŒ€ì²´ë¡œ ì´ì „ í‚¤ì›Œë“œ í™œìš©
    extracted_keywords = extract_keywords(query, user_feedback_list, max_keywords=3)

    # í‚¤ì›Œë“œ ê¸°ë°˜ ìµœì¢… ê²€ìƒ‰ì–´
    if extracted_keywords:
        query = " ".join(extracted_keywords)  # ì¶”ì¶œëœ í‚¤ì›Œë“œë¡œ ìƒˆ ì¿¼ë¦¬ 
        return (extracted_keywords, query)
    else: # ì¶”ì¶œëœ í‚¤ì›Œë“œ ì—†ë‹¤ë©´
        return (None, None)  # ì´ˆê¸° ì¿¼ë¦¬ ì„¤ì • í•„ìš”


# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(query:str, user_feedback_list:str, max_keywords:int=3) -> List:
    """
        query: ìµœì¢… ê²€ìƒ‰ì–´. (ìµœì‹ )user_feedbackì´ "NOFEEDBACK"ì¼ ê²½ìš° ì‚¬ìš©.
        user_feedback_list : ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ ë‚´ì—­.
        max_keywords : ë°˜í™˜í•  ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜.
        Returns value: ì¶”ì¶œëœ SEO í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸.
    """
    fail_cnt = 0  # ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”

    while fail_cnt < 3:
        try:
            # system(gpt) ì—­í•  í”„ë¡¬í”„íŒ…
            system_prompt = f"""
            ë‹¹ì‹ ì˜ ì—­í• ì€ SEO(ê²€ìƒ‰ ì—”ì§„ ìµœì í™”)ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

            1. **í‚¤ì›Œë“œ ìƒì„± ì¡°ê±´**:
              - ìµœì‹  í”¼ë“œë°±(ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¸ë±ìŠ¤ê°€ ë†’ì€ ìˆœì„œ)ì„ ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥ì„±ì´ ë†’ì€ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
              - ìµœì‹  í”¼ë“œë°±ì´ 'NOFEEDBACK'ì¸ ê²½ìš°:
                - ê¸°ì¡´ ì¿¼ë¦¬(query)ë¥¼ ì°¸ê³ í•˜ì—¬ ë” ì„¸ë¶€ì ì´ê³  êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
                - ê¸°ì¡´ ì¿¼ë¦¬ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
              - ì‚¬ìš©ì í”¼ë“œë°± ë¦¬ìŠ¤íŠ¸ê°€ ì „ë¶€ 'NOFEEDBACK'ì¸ ê²½ìš°:
                ë‹¤ìŒ ì£¼ì œ ì¤‘ í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì„¸ìš”:
                  <ì—­ì‚¬,ì² í•™,ê³¼í•™,ì˜ˆìˆ ,ê¸°ìˆ ,ë¬¸í™”,ê±´ê°•>
                - ì„ íƒí•œ ì£¼ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥ì„±ì´ ë†’ì€ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
              - ì¿¼ë¦¬ê°€ 'NOARTICLE'ë¡œ ëë‚˜ë©´:
                - ì¿¼ë¦¬ì˜ ë§ˆì§€ë§‰ì„ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë” ì¼ë°˜ì ì´ê³  í¬ê´„ì ì¸ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

            2. **í‚¤ì›Œë“œ ìƒì„± ê·œì¹™**:
              - ê²€ìƒ‰ ì—”ì§„ì—ì„œ ìì£¼ ê²€ìƒ‰ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”.
              - ëª…ì‚¬ ì¤‘ì‹¬ì˜ êµ¬ì²´ì ì´ê³  ì§ê´€ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
              - ë™ì¼í•œ ë‹¨ì–´ë‚˜ ì¤‘ë³µëœ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ì„¸ìš”.
              - í‚¤ì›Œë“œì˜ ê°œìˆ˜ëŠ” {max_keywords}ê°œ ì…ë‹ˆë‹¤

            3. **ì¶œë ¥ í˜•ì‹**:
              - ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”. 
                ì˜ˆ:
                ```
                  {{"k1":"í‚¤ì›Œë“œ1", "k2":"í‚¤ì›Œë“œ2", "k3":"í‚¤ì›Œë“œ3", ...}}
                ```
            """

            # Open API í˜¸ì¶œ
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": (
                            f"ì‚¬ìš©ì í”¼ë“œë°± (ìµœì‹  í”¼ë“œë°± ìš°ì„  ì •ë ¬): {user_feedback_list}\n"
                            f"ì¿¼ë¦¬: {query}\n\n"
                            "í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
                        ),
                    },
                ],
                temperature=0,
                max_tokens=2048,
                top_p=1, # ì¶”ì¶œ ë‹¨ì–´ì˜ ê´€ë ¨ì„± ì œì–´
                frequency_penalty=0, # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë¥¼ ì œì–´(ê³ ë ¤)
                presence_penalty=0, # ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì œì–´(ê³ ë ¤)
            )

            # GPTì˜ ì‘ë‹µ ë¶„ì„ 
            keywords = response["choices"][0]["message"]["content"]

            # (JSON -> ë”•ì…”ë„ˆë¦¬) ë³€í™˜
            keywords = keywords.replace("'", "\"") # JSON í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
            try:
                keywords_dict = json.loads(keywords)
                keywords_list = list(keywords_dict.values()) # (ë”•ì…”ë„ˆë¦¬ value -> ë¦¬ìŠ¤íŠ¸) ë³€í™˜
            except json.JSONDecodeError as e:
                fail_cnt += 1
                print(f"ğŸ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì‘ë‹µ ë‚´ìš©: {response['choices'][0]['message']['content']}")
                continue  # ì¬ì‹œë„

            # ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ 
            return keywords_list

        except openai.error.RateLimitError:
            fail_cnt += 1
            print("ğŸ” Rate limitì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. 40ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
            time.sleep(40)
        except Exception as e:
            print(f"ğŸ” Error during OpenAI API call: {e}")
            return []

    print("âš ï¸ 3ë²ˆ ì´ìƒì˜ ì‹¤íŒ¨ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    return []  # 3ë²ˆ ì´ìƒ ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜






'''
    select_article
        - Google_API
        - process_recommend_article
            - get_article_body
            - find_recommend_article
            - Google_API
        
'''
# ì‚¬ìš©ì ì…ë ¥ 
def select_article(user:User, query:str, user_feedback_list:list) -> Dict:

    num_results_per_site = 3    # ê° ì‚¬ì´íŠ¸ë‹¹ ê²°ê³¼ ê°œìˆ˜
    sites = [                   # ê²€ìƒ‰ ê°€ëŠ¥ ì‚¬ì´íŠ¸ ëª©ë¡ 
        # "brunch.co.kr",
        "bbc.com",
        "khan.co.kr",
        "hani.co.kr",
        "ytn.co.kr",
        "sisain.co.kr",
        "news.sbs.co.kr",
        "h21.hani.co.kr",
        "ohmynews.com",
    ]

    # í›„ë³´ ê¸°ì‚¬ ëª©ë¡ ì„œì¹˜ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê¸°ë°˜ ì¿¼ë¦¬ë¡œ)
    df = Google_API(user, query, num_results_per_site, sites)  # queryë¡œ íƒìƒ‰ëœ ê¸°ì‚¬ ëª©ë¡
    time.sleep(30)  # ìƒì„± í† í° ì œí•œ ì—ëŸ¬ ì˜ˆë°©

    
    # ì¶”ì²œ ì•„í‹°í´ ê²°ì • # ë™ì¼ ì•„í‹°í´ ì¶”ì²œ ë°©ì§€ í•„ìš” -> cache ì ìš©
    extracted_keywords = None
    while True:
        # ì¶”ì²œ ì•„í‹°í´
        info_for_the_article = process_recommend_article(df, user_feedback_list)

        if info_for_the_article is None or info_for_the_article.empty: # ì¶”ì²œëœ ì•„í‹°í´ì´ ì—†ê±°ë‚˜ ë³¸ë¬¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•  ê²½ìš°
            # ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìƒì„±í•˜ì—¬ ì¿¼ë¦¬(ê²€ìƒ‰ì–´) ì¬êµ¬ì„±
            if "NOARTICLE" not in query:  # ì¤‘ë³µ ì¶”ê°€ ë°©ì§€
                query = " ".join("NOARTICLE") # "NOARTICLE"ì„ ê¸°ì¡´ queryì— ì¶”ê°€

                # í‚¤ì›Œë“œ ì¶”ì¶œ
                extracted_keywords = extract_keywords(query, user_feedback_list, max_keywords=3)
                if extracted_keywords:
                    query = " ".join(extracted_keywords) # ì¶”ì¶œëœ í‚¤ì›Œë“œ ì €ì¥(ê¸°ì¡´ í‚¤ì›Œë“œ ì‚­ì œ & ìƒˆë¡œìš´ ê²€ìƒ‰ì–´ ì„¤ì •)
                else:
                    query = None

                # Google APIë¡œ ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰
                df = Google_API(user, query, num_results_per_site=5, sites=sites)
                if df.empty: # ìƒˆë¡œìš´ ê²€ìƒ‰ì–´ë¡œë„ ê²°ê³¼ë¥¼ ì°¨ì§€ ëª»í•¨
                    continue  # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ë°˜ë³µ
            else: # ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨. 
                break # ë£¨í”„ ì¢…ë£Œ
        else: # ì¶”ì²œ ì•„í‹°í´ì´ ì¡´ì¬í•œë‹¤ë©´
            # Title, URL ë° Body ì¶”ì¶œ
            recommend_article_title = info_for_the_article.iloc[0]["Title"]
            recommend_article_body = info_for_the_article.iloc[0]["Body"]
            recommend_article_url = info_for_the_article.iloc[0]["URL"]
            recommend_article_reason = info_for_the_article.iloc[0]["Reason"]

            # ë³¸ë¬¸ì´ ìœ íš¨í•œì§€ í™•ì¸
            # IndexError: single positional indexer is out-of-bounds -> recommend_article_body (DataFrame)ì´ ë¹ˆ ê²½ìš° ì¢…ì¢… ë°œìƒ!
            if recommend_article_body and len(recommend_article_body.strip()) > 0:
                print("âœ… ì¶”ì²œ ì•„í‹°í´ URL:", recommend_article_url)
                break  # ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
    
    return  {
        "title": recommend_article_title,
        "body": recommend_article_body, 
        "url": recommend_article_url, 
        "reason": recommend_article_reason, 
        "retry_extracted_keywords": extracted_keywords # í‚¤ì›Œë“œ ì¬ì¶”ì¶œì‹œ, DBì— ë°˜ì˜í•˜ê¸° ìœ„í•¨ 
    }


        



# í›„ë³´ ê¸°ì‚¬ë“¤ ("Title", "Description", "Link", "Domain") í˜•ì‹ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ
def Google_API(user:User, query:str, num_results_per_site:int, sites:list[str]) -> pd.DataFrame: # DataFrame:2ì°¨ì› ë°ì´í„° êµ¬ì¡° (í–‰&ì—´ êµ¬ì„±)
    # ê° ì‚¬ì´íŠ¸ì˜ ê²°ê³¼ ëª¨ìŒ ë¦¬ìŠ¤íŠ¸(ì¶”ì²œ ê¸°ì‚¬ í›„ë³´ ë¦¬ìŠ¤íŠ¸)
    df_google_list = [] # ìš”ì†ŒëŠ” dataframeìœ¼ë¡œ í•œ ê¸°ì‚¬ì˜ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ

    # ì‚¬ìš©ìì˜ ê³¼ê±° ì•„í‹°í´ ì •ë³´ # ìœ ì € ì •ë³´ íŒŒë¼ë¯¸í„° í•„ìš”
    past_articles = set(Article.objects.filter(user=user).order_by('-timestamp')[:100].values_list('url', flat=True)) # userì˜ ê³¼ê±° ì•„í‹°í´ì„ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ í›„, ìƒìœ„ 100ê°œ ë°˜í™˜(ì¤‘ë³µì€ ì‚­ì œ)


    # ì‚¬ì´íŠ¸ ë³„ ì¿¼ë¦¬ë¡œ í›„ë³´ ê¸°ì‚¬ ì¡°íšŒ
    for site in sites: 
        
        site_query = f"site:{site} {query}"     # ê° ì‚¬ì´íŠ¸ ë³„ ê²€ìƒ‰ì–´ êµ¬ì„±
        collected_results_cnt = 0               # í˜„ì¬ ì‚¬ì´íŠ¸ì—ì„œ ìˆ˜ì§‘í•œ ê²°ê³¼ ìˆ˜
        start_index = 1                         # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ íƒìƒ‰ ì‹œì‘í•  ìœ„ì¹˜ë¥¼ ì§€ì •
        num = 5                                 # í•œ ë²ˆì— ë°˜í™˜í•  ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆ˜
        # Google Custom Search APIì— ëŒ€í•œ ìš”ì²­ URL ìƒì„±
        url = f"https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={GOOGLE_SEARCH_ENGINE_ID}&q={site_query}&start={start_index}&num={num}" 
            
        while collected_results_cnt < num_results_per_site: # ì •í•´ì§„ ê°œìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ    
            # ìš”ì²­
            try:
                # URLë¡œ HTTP GET ìš”ì²­
                response = requests.get(url)

                # ìš”ì²­ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                if response.status_code != 200: # Get ìš”ì²­ ì‹¤íŒ¨
                    print(f"âš ï¸ {site}ì— ëŒ€í•œ HTTP GET ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, Message: {response.text}")
                    break 
                
                # API ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬(ìš”ì²­ ì„±ê³µì‹œ ì‘ë™)
                data = response.json() # APIì˜ ì‘ë‹µ ë°ì´í„° -> JSON í˜•ì‹
                # print(f"get data: {data}") # ë””ë²„ê¹…ìš© # êµ¬ì¡° í™•ì¸ìš© 
                
                # ê²€ìƒ‰ ê²°ê³¼ í•­ëª© ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                search_items = data.get("items") # ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ë“¤ ê°€ì ¸ì˜¤ê¸°
                if not search_items: # ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´
                    print(f"ğŸ” No more results found for site {site}.")
                    break 

                # ê²€ìƒ‰ ê²°ê³¼ ìˆœíšŒ(ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ)
                for search_item in search_items:
                    # ê²°ê³¼ ê°œìˆ˜ ì²´í¬
                    if collected_results_cnt >= num_results_per_site: # ì •í•´ì§„ ê°œìˆ˜ì— ë„ë‹¬í•œ ê²½ìš°
                        break # for ë£¨í”„ ì¢…ë£Œ

                    # ì •ë³´ ì¶”ì¶œ
                    link = search_item.get("link")
                    title = search_item.get("title")
                    description = search_item.get("snippet")

                    # ì‚¬ìš©ì ê³¼ê±° ì•„í‹°í´ê³¼ì˜ ì¤‘ë³µ ë°©ì§€
                    if link in past_articles:
                        continue  

                    # ì¶”ê°€ ì¡°ê±´: m.khan.co.kr ì²˜ë¦¬
                    if site == "khan.co.kr" and "m.khan.co.kr" in link:
                        link = link.replace("m.khan.co.kr", "khan.co.kr") # (ëª¨ë°”ì¼ ë§í¬ -> ì¼ë°˜ ì›¹ì‚¬ì´íŠ¸ ë§í¬)ë³€í™˜

                    # ê¸°ì‚¬ ì •ë³´ì— ëŒ€í•œ DataFrame ìƒì„± í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    df_google_list.append(
                        pd.DataFrame(
                            [[title, description, link, site]], # ë‹¤ìŒ í–‰ ì¶”ê°€(í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ 1ê°œì˜ í–‰ êµ¬ì„±)
                            columns=["Title", "Description", "Link", "Domain"], # ì—´ ì •ë³´
                        )
                    )
                    collected_results_cnt += 1  # í˜„ì¬ ìˆ˜ì§‘ ê²°ê³¼ ê°œìˆ˜ ì¦ê°€
                # ë‹¤ìŒ í˜ì´ì§€ ê²€ìƒ‰
                if num_results_per_site > 10: # num_results_per_siteê°€ 10 ì´ìƒì¼ ë•Œë§Œ
                    start_index += 10 # start_indexë¥¼ ì¦ê°€ì‹œí‚´(numì„ 10ìœ¼ë¡œ ì„¤ì •í•´ë’€ê¸° ë•Œë¬¸)

            except Exception as e:
                print(f"âš ï¸ Error occurred for site {site}: {e}")
                break

    # ëª¨ë“  ì‚¬ì´íŠ¸ì˜ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
    if df_google_list: 
        # df_google_listì˜ ìš”ì†Œì¸ ê° DataFrameë“¤ì€ ëª¨ë‘ ë™ì¼í•œ ì—´ êµ¬ì¡°ì´ë¯€ë¡œ, í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•© ê°€ëŠ¥
        df_google = pd.concat(df_google_list, ignore_index=True)  # ignore_index=True: ìƒˆë¡œìš´ ì—°ì†ì ì¸ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬
    else:  # df_google_listê°€ ë¹„ì–´ìˆë‹¤ë©´
        df_google = pd.DataFrame(columns=["Title", "Description", "Link", "Domain"]) # ë¹ˆ DataFrame 

    return df_google
   



# ì¶”ì²œëœ ì•„í‹°í´ì—ì„œ URL, Body, Titleì„ ì¶”ì¶œ
    # ì¶”ì²œëœ ì•„í‹°í´ì´ ì—†ê±°ë‚˜, ì¶”ì²œëœ ì•„í‹°í´ì˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì„ ê²½ìš° 
    # í•´ë‹¹ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œìš´ ì¶”ì²œì„ ìš”ì²­í•˜ëŠ” í•¨ìˆ˜
def process_recommend_article(df:pd.DataFrame=None, user_feedback:str="") -> pd.DataFrame:
    # ì´ˆê¸°í™”
    recommend_article = pd.DataFrame(columns=["Title", "Description", "Link", "Domain"])
    fail = 0

    # ì¶”ì²œ ì •ë³´ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤
    while fail < 6:
        # ì¶”ì²œ ì•„í‹°í´ íƒìƒ‰
        recommend_article, reason = find_recommend_article(df, user_feedback) 

        # ì•„í‹°í´ ì¡´ì¬ ì—¬ë¶€ íŒŒì•…
        if recommend_article.empty: # ì¶”ì²œëœ ì•„í‹°í´ì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            print("ì¶”ì²œëœ ì•„í‹°í´ì´ ë” ì´ìƒ ì—†ìŠµë‹ˆë‹¤.")
            return None #  while ë£¨í”„ ì¢…ë£Œ

        try: # ì¶”ì²œ ì•„í‹°í´ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            # URL, Domain, Title ì¶”ì¶œ
            url = recommend_article.iloc[0]["Link"]  # URL ì¶”ì¶œ
            domain = recommend_article.iloc[0]["Domain"]  # Domain ì¶”ì¶œ
            title = recommend_article.iloc[0]["Title"]  # Title ì¶”ì¶œ
        except IndexError as e:
            print(f"ğŸ” ì¶”ì²œëœ ì•„í‹°í´ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            recommend_article.drop(recommend_article.index[0], inplace=True) # recommend_articleì—ì„œ ì‚­ì œ
            df.drop(df[df["Link"] == url].index, inplace=True) # dfì—ì„œ ì‚­ì œ
            fail += 1
            continue  # ìƒˆë¡œìš´ ì•„í‹°í´ì„ íƒìƒ‰ì„ ìœ„í•´ while ë£¨í”„ ì¬ì‹œì‘

        # ë³¸ë¬¸(article body) ì¶”ì¶œ
        article_body = get_article_body(url, domain)  # ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ
        
        # ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ ë³¸ë¬¸ ê¸¸ì´ê°€ 5ë¬¸ì¥ ì´í•˜ì¸ ê²½ìš° ì²˜ë¦¬
        if ( not article_body or len([s for s in article_body.split(".") if s.strip()]) <= 5 ):
            print(f"ğŸ” ë³¸ë¬¸ì´ ì—†ëŠ” ì•„í‹°í´ (ë˜ëŠ” ë³¸ë¬¸ì´ 5ë¬¸ì¥ ì´í•˜)ì´ë¯€ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            recommend_article.drop(recommend_article.index[0], inplace=True)# recommend_articleì—ì„œ ì‚­ì œ
            df.drop(df[df["Link"] == url].index, inplace=True) # dfì—ì„œ ì‚­ì œ
            fail += 1
            continue   # ìƒˆë¡œìš´ ì•„í‹°í´ì„ íƒìƒ‰ì„ ìœ„í•´ while ë£¨í”„ ì¬ì‹œì‘

        # ë³¸ë¬¸ì´ ìœ íš¨í•  ê²½ìš° DataFrame ìƒì„± ë° ë°˜í™˜
        info_for_the_article = pd.DataFrame(
            [[title, url, article_body, reason]],
            columns=["Title", "URL", "Body", "Reason"]
        )
        return info_for_the_article
    
    info_for_the_article = pd.DataFrame(
            [["ì‹¤íŒ¨", "ì‹¤íŒ¨", "ì‹¤íŒ¨", "ì‹¤íŒ¨"]],
            columns=["Title", "URL", "Body", "Reason"]
    )
    return info_for_the_article
    


# ì¶”ì²œ ì•„í‹°í´ ê²°ì •#
def find_recommend_article(df_google:pd.DataFrame, user_feedback_list:list) -> Tuple[pd.DataFrame, str]:
    fail = 0
    # ì•„í‹°í´ ëª©ë¡ì— index í¬í•¨
    article_titles = df_google["Title"].tolist()
    article_descriptions = df_google["Description"].tolist()
    article_indices = df_google.index.tolist()  # DataFrameì˜ indexë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥

    while fail < 3:  
        try:
            # Open API í˜¸ì¶œ
                # í† í° ì´ˆê³¼ ì—ëŸ¬ ë°œìƒí•´ì„œ, title ì •ë³´ëŠ” ì œì™¸í•¨!
            system_prompt = f"""
            # ì§€ì‹œë¬¸
                - ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ í”¼ë“œë°±ê³¼ ì•„í‹°í´ì˜ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì•„í‹°í´ì„ ì¶”ì²œí•˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì˜ ì—­í• ì„ í•œë‹¤.
            # ì¶”ì²œ ì¡°ê±´
                1. ìµœì‹  í”¼ë“œë°±(ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¸ë±ìŠ¤ê°€ ë†’ì€ ìˆœì„œ)ì„ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.
                2. ìµœì‹  í”¼ë“œë°±ì´ ë‹¤ë£¨ëŠ” ì£¼ì œì™€ ê°€ì¥ ê´€ë ¨ì´ ìˆëŠ” ì•„í‹°í´ì„ ì„ íƒí•˜ì„¸ìš”.
                3. ì œëª©ê³¼ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„í‹°í´ì˜ ì í•©ì„±ì„ íŒë‹¨í•˜ì„¸ìš”.
                4. ë‹¨ìˆœ ë‰´ìŠ¤ ë³´ë„, ê´‘ê³ ì„± ë‚´ìš©, ë˜ëŠ” ì¤‘ë³µëœ ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”.
                5. ì§€ì‹ì ì¸ ì„¤ëª… ë˜ëŠ” í•™ìŠµì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.

            # ì¶œë ¥ í˜•ì‹
              - ë‹µë³€ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”. 
              - ë”•ì…”ë„ˆë¦¬ key ì´ë¦„ì€ "index"ì™€ "reason"ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.
              - "reason" keyì˜ valueì— í•´ë‹¹í•˜ëŠ” ë¬¸ìì—´ ë‚´ë¶€ì—ì„œ ì‘ì€ë”°ì˜´í‘œ(')ì™€ í°ë”°ì˜´í‘œ(")ê°€ ë“±ì¥í•˜ì§€ ì•Šë„ë¡ ë¬¸ìì—´ì„ êµ¬ì„±í•˜ì„¸ìš”.
              - "reason" keyì˜ valueëŠ” 2ë¬¸ì¥ ì •ë„ì˜ í•œêµ­ì–´ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
                ì˜ˆ:
                ```
                    {{"index": "ì¶”ì²œëœ ì•„í‹°í´ì˜ ê³ ìœ  index", "reason": "ì™œ ì´ ì•„í‹°í´ì´ ì í•©í•œì§€ ê°„ë‹¨íˆ ì„¤ëª…" }}
                ```
            """
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    { "role": "system", "content": system_prompt,},
                    {   "role": "user",
                        "content": (
                            f"ì‚¬ìš©ì í”¼ë“œë°±: {user_feedback_list}\n\n"
                            "ì•„í‹°í´ ëª©ë¡ (index í¬í•¨):\n"
                            + "\n".join(
                                f"{i}. [Index: {idx}]  ì„¤ëª…: {description}\n  "
                                for i, (idx, description) in enumerate(
                                    zip(
                                        article_indices,
                                        article_descriptions,
                                    )
                                )
                            )
                        ),
                    },
                ],
                temperature=0,
                max_tokens=2048,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0,
            )


            # GPTì˜ ì‘ë‹µ ë¶„ì„ 
            content = response["choices"][0]["message"]["content"]
            # (JSON -> ë”•ì…”ë„ˆë¦¬) ë³€í™˜ ì‘ì—…
            content = re.sub(r'"reason":\s*"([^"]*?)"', escape_inner_quotes, content)
            try:
                content_dict = json.loads(content)
            except json.JSONDecodeError as e:
                print(f"ğŸ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì‘ë‹µ ë‚´ìš©: {response['choices'][0]['message']['content']}")
                fail += 1 
                continue

            # content ì¡´ì¬ ê²€ì¦
            if not isinstance(content_dict["index"], int): 
                return (pd.DataFrame(), "")
            elif not isinstance(content_dict["reason"], str) or not content_dict["reason"]: # íƒ€ì…ê³¼ ì¡´ì¬ ì—¬ë¶€
                return (pd.DataFrame(), "")

            # ì •ìˆ˜ index ì €ì¥ 
            recommended_index = int(content_dict["index"])  
            # indexê°€ DataFrameì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if recommended_index not in df_google.index:
                print(f"ğŸ” ì¶”ì²œëœ index({recommended_index})ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return (pd.DataFrame(), "")

            # í•´ë‹¹ indexë¡œ í–‰(í•´ë‹¹ ê¸°ì‚¬ ì •ë³´) ë°˜í™˜
            recommended_article = df_google.loc[[recommended_index]] 
            reason = content_dict["reason"]
            return (recommended_article, reason) # ê²°ê³¼ DataFrame í˜•íƒœë¡œ ë°˜í™˜

        except openai.error.RateLimitError:
            print("ğŸ” Rate limit reached. Retrying in 40 seconds...")
            fail += 1
            time.sleep(40)  # 40ì´ˆ ì§€ì—° í›„ ì¬ì‹œë„
            continue  #

    return (pd.DataFrame(), "") # ìµœëŒ€ 3ë²ˆ ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ 
    


def get_article_body(url:str, domain:str) -> str:
    # ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì´íŠ¸ë³„ íƒœê·¸ ì •ë³´#
    SITE_CLASS_MAPPING = {
        # "brunch.co.kr": [{"tag": "div", "class": "wrap_body"}],
        "bbc.com": [{"tag": "main", "class": "bbc-fa0wmp"}],
        "khan.co.kr": [{"tag": "div", "class": "art_body"}],
        "hani.co.kr": [{"tag": "div", "class": "article-text"}],
        "ytn.co.kr": [{"tag": "div", "class": "vodtext"}],
        "sisain.co.kr": [{"tag": "div", "class": "article-body"}],
        "news.sbs.co.kr": [{"tag": "div", "class": "main_text"}],
        "h21.hani.co.kr": [{"tag": "div", "class": "arti-txt"}],
        "ohmynews.com": [
            {"tag": "article", "class": "article_body at_contents article_view"},
            {"tag": "div", "class": "news_body"},
        ],
        # ê³µë°± ì£¼ì˜ # ì¶”ê°€ ë„ë©”ì¸ ë° íƒœê·¸/í´ë˜ìŠ¤ ë§¤í•‘
        # í•œêµ­ì¼ë³´, mbcnewsëŠ” ì œì™¸
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    }

    try:
        # URLì—ì„œ HTML (ìš”ì²­)ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, headers=headers)
        response.raise_for_status() # ì˜¤ë¥˜ ë°œìƒí•˜ë©´, ì˜ˆì™¸ ë°œìƒ
        soup = BeautifulSoup(response.text, "html.parser")

        # ë„ë©”ì¸ì´ ì œê³µëœ ê²½ìš° SITE_CLASS_MAPPINGì—ì„œ ì²˜ë¦¬
        site_info = SITE_CLASS_MAPPING.get(domain)
        if not site_info: # í•´ë‹¹ ë„ë©”ì¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° 
            return None

        # ëª¨ë“  ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒí•˜ë©° íƒœê·¸/í´ë˜ìŠ¤ ì²˜ë¦¬
        for mapping in site_info:
            tag_name = mapping.get("tag")
            class_name = mapping.get("class")
            main_body = soup.find(tag_name, class_=class_name)
            if main_body:
                # íƒœê·¸ ë‚´ë¶€ì— p, h1 ë“±ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
                text_elements = main_body.find_all(["h1", "h2", "h3", "h4", "p", "li"])
                # <p> íƒœê·¸ ê°œìˆ˜ í™•ì¸ (2ê°œ ì´í•˜ì´ë©´ ë³¸ë¬¸ì´ ë¶€ì¡±í•˜ë‹¤ê³  ê°„ì£¼)

                paragraph_count = len(main_body.find_all("p"))
                if paragraph_count <= 2:
                    return main_body.get_text(strip=True)

                if text_elements:
                    return "\n".join(
                        [element.get_text(strip=True) for element in text_elements]
                    )
                else:
                    return main_body.get_text(strip=True)
        # í•´ë‹¹ ë„ë©”ì¸ì— ë§¤í•‘ëœ íƒœê·¸ì™€ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° 
        return None 
    except requests.exceptions.RequestException as e: # HTTP ìš”ì²­ ì˜¤ë¥˜
        return None
    except Exception as e: # ë³¸ë¬¸ ì¶”ì¶œì¤‘ ì˜¤ë¥˜
        return None



# ì •ê·œì‹: "key": "value"
# keyë‚˜ ì „ì²´ êµ¬ì¡°ì— ìˆëŠ” í°ë”°ì˜´í‘œëŠ” ê±´ë“œë¦¬ë©´ ì•ˆ ë˜ë¯€ë¡œ value ë‚´ë¶€ë§Œ ë°”ê¿”ì•¼ í•¨
def escape_inner_quotes(match):
    inner = match.group(1)
    escaped = re.sub(r'"', r'\\"', inner)  # í°ë”°ì˜´í‘œ escape
    return f'"reason": "{escaped}"'